We introduce \tit{partial lexicographic preference trees} (PLP-trees) 
as a formalism for compact representations of preferences over 
combinatorial domains. Our main results concern the problem of passive 
learning of PLP-trees. Specifically, for several classes of 
PLP-trees, we study how to learn (i) a PLP-tree consistent with a 
dataset of examples, possibly subject to requirements on the size 
of the tree, and (ii) a PLP-tree correctly ordering as many of 
the examples as possible in case the dataset of examples is inconsistent. 
We establish complexity of these problems and, in all cases where the 
problem is in the class P, propose polynomial time algorithms.

\section{Introduction}
%Representing and reasoning about preferences are fundamental to decision
%making and so, of significant interest to artificial intelligence. When 
%the choice is among a few \emph{outcomes} (\emph{alternatives}),
%representations in terms of explicit enumerations of the preference order 
%are feasible and lend themselves well to formal analysis. However, in many
%applications outcomes come from \emph{combinatorial domains}. That is,
%outcomes are described as tuples of values of \emph{attributes} (also 
%referred to as \emph{variables} or \emph{attributes}), say $X_1,\ldots, 
%X_p$, with each attribute $X_i$ assuming values from some set $D_i$ -- its 
%\emph{domain}. 
%Because of the combinatorial size of the space of such outcomes, explicit
%enumerations of their elements are impractical. Instead, we resort to 
%formalisms supporting intuitive and, ideally, concise \emph{implicit}
%descriptions of the order. The language of CP-nets \cite{bbdh03} is a prime
%example of such a formalism. 

Recently, there has been a rising interest in representing
preferences over combinatorial domains by exploiting the notion of
the lexicographic ordering. For instance, assuming attributes are over the  
binary domain $\{0,1\}$, with the preferred value for each attribute being 
1, a sequence of attributes naturally determines an order on outcomes.
This idea gave rise to the language of \tit{lexicographic preference 
models} or \tit{lexicographic strategies}, which has been extensively 
studied in the literature \cite{schmitt2006complexity,dombi2007learning,%
yaman2008democratic}. The formalism of complete \tit{lexicographic 
preference trees} (LP-trees) \cite{booth:learningLP} generalizes the
language of lexicographic strategies by arranging attributes into 
decision trees that assign preference ranks to outcomes. An important
aspect of LP-trees is that they allow us to model \emph{conditional}
preferences on attributes and \emph{conditional} ordering of attributes. 
Another formalism,
the language of \tit{conditional lexicographic preference trees} (or 
CLP-trees) \cite{brauning2012learning}, extends LP-trees by allowing 
subsets of attributes as labels of nodes.

A central problem in preference representation concerns learning 
implicit models of preferences (such as lexicographic strategies,
LP-trees or CLP-trees), of possibly small sizes, that are consistent 
with all (or at least possibly many) given examples, each correctly 
ordering a pair of outcomes. The problem was extensively studied. Booth
et al. \shortcite{booth:learningLP} considered learning of LP-trees, and 
Br\"auning and Eyke \shortcite{brauning2012learning} of CLP-trees. 

In this work, we introduce \emph{partial lexicographic preference trees} (or 
\emph{PLP-trees}) as means to represent \emph{total preorders} over 
combinatorial domains. PLP-trees are closely related to LP-trees 
requiring that every path in the tree contains all attributes 
used to describe outcomes. Consequently, LP-trees describe total 
orders over the outcomes. PLP-trees relax this requirement and allow 
paths on which some attributes may be missing. Hence, 
PLP-trees describe total preorders. This seemingly small difference
has a significant impact on some of the learning problems. It allows
us to seek PLP-trees that minimize the set of attributes on their paths,
which may lead to more robust models by disregarding attributes 
that have no or little influence on the true preference (pre)order.

The rest of the chapter is organized as follows. In the next section, 
we introduce the language of PLP-trees and describe a classification 
of PLP-trees according to their complexity. We also define three types of passive 
learning problems for the setting of PLP-trees. In the following 
sections, we present algorithms learning PLP-trees of particular
types and computational complexity results on the existence of 
PLP-trees of different types, given size or accuracy. We close 
with conclusions and a brief account of future work.


\section{Partial Lexicographic Preference Trees}
Let $\cI=\{X_1,\ldots,X_p\}$ be a set of binary attributes, with each
$X_i$ having its domain $D_i=\{0_i, 1_i\}$. The corresponding 
\tit{combinatorial domain} is the set $\cX=D_1 \times \ldots \times D_p$.
Elements of  $\cX$ are called \tit{outcomes}.

A PLP-tree over $\cX$ is binary tree whose every
non-leaf node is labeled by an attribute from $\cI$ and by a preference
entry $1>0$ or $0>1$, and whose every leaf node is denoted by a box 
$\Box$. Moreover, we require that on every path from the root to a leaf
each attribute appears \emph{at most} once. 

To specify the total preorder on outcomes defined by a PLP-tree $T$, let 
us enumerate leaves of $T$ from left to right, assigning them 
integers $1,2$, etc. For every outcome $\alpha$ we find its leaf 
in $T$ by starting at the root of $T$ and proceeding downward. 
When at a node labeled with an attribute $X$, we descend to the left or to the 
right child of that node based on the value $\alpha(X)$ of the attribute $X$ 
in $\alpha$ and on the preference assigned to that node. If $\alpha(X)$ is 
the preferred value, we descend to the left child. We descend to the right
child,
otherwise. The integer assigned to the leaf that we eventually get 
to is the \emph{rank} 
of $\alpha$ in $T$, written $r_T(\alpha)$. The preorder $\succeq_T$ on 
distinct outcomes determined by $T$ is defined as follows: $\alpha\succeq_T \beta$ 
if $r_T(\alpha)\leq r_T(\beta)$ (smaller ranks are ``better''). We also 
define derived relations $\succ_T$ (strict order) and $\approx_T$ 
(equivalence or indifference): $\alpha \succ_T\beta$ if $\alpha\succeq_T
\beta$ and $\beta \not\succeq_T \alpha$, and $\alpha_T\approx_T\beta$ if 
$\alpha\succeq_T\beta$ and $\beta\succeq_T \alpha$. Clearly, $\succeq_T$ 
is a total preorder on outcomes partitioning them into strictly ordered 
clusters of equivalent outcomes. 
%To simplify the notation, we always drop
%the sbscript $T$  whenever it is clear from the context.

To illustrate the notions just introduced, we consider preference orderings 
of car options over four binary attributes. The \tit{capacity} ($X_1$) can 
be either \tit{low} ($0_1$) or \tit{high} ($1_1$). The \tit{price} 
($X_2$) is either \tit{low} ($0_2$) or \tit{high} ($1_2$).
The \tit{safety} ($X_3$) can be \tit{low} ($0_3$) or \tit{high} 
($1_3$). Finally, the \tit{transmission} ($X_4$) of a car 
can be \tit{manual} ($0_4$) or \tit{automatic} ($1_4$).
An agent could specify her preferences over cars as a PLP-tree in 
\figref{UICP_PLPT_full}. \emph{Price} is the most important attribute to 
the agent and she prefers high to low. Her next most important attribute is 
\emph{capacity} (independently of her selection for price). She 
prefers high over low on capacity for expensive cars, 
and low over high for inexpensive cars.
Among the expensive cars, no matter what capacity she considers, her next 
consideration is the \emph{transmission}, and she prefers automatic to manual.
In this example, the attribute \emph{safety} does not figure into preferences at all.
The most preferred cars are automatic cars with high price and high capacity,
with all possible combinations of choices for safety (and so, 
the cluster of most preferred cars has two elements).

\begin{figure}[!ht]
	\centering
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level 1/.style={sibling distance=2cm, level distance=25pt},
			level 2/.style={sibling distance=0.7cm, level distance=25pt},
			level 3/.style={sibling distance=0.4cm, level distance=25pt}]
	    \node [main node,inner sep=0.5pt,label={[xshift=0.7cm, yshift=-0.5cm]$\scriptstyle 1_2>0_2$}] (1){$X_2$}
	      child {node [main node,inner sep=0.5pt,label={[xshift=-0.7cm, yshift=-0.5cm]$\scriptstyle 1_1>0_1$}] (3) {$X_1$}
	      	child {node [main node,inner sep=0.5pt,label={[xshift=-0.65cm, yshift=-0.5cm]$\scriptstyle 1_4>0_4$}] (7) {$X_4$}
	      		child {node [rectangle,draw] (8) {}}
	      		child {node [rectangle,draw] (9) {}}
					}
	      	child {node [main node,inner sep=0.5pt,label={[xshift=0.7cm, yshift=-0.5cm]$\scriptstyle 1_4>0_4$}] (10) {$X_4$}
	      		child {node [rectangle,draw] (11) {}}
	      		child {node [rectangle,draw] (12) {}}
					}
				}
	      child {node [main node,inner sep=0.5pt,label={[xshift=0.7cm, yshift=-0.5cm]$\scriptstyle 0_1>1_1$}] (6) {$X_1$}
	    		child {node [rectangle,draw] (4) {}}
	    		child {node [rectangle,draw] (5) {}}
	      };
	  \end{tikzpicture}
		\caption{Collapsible PLP-tree \label{fig:UICP_PLPT_full}}
	\end{subfigure}%
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level/.style={sibling distance=2cm/#1, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$X_2$}
	      child {node [main node,inner sep=0.5pt] (2) {$X_1$}
	      	child {node [main node,inner sep=0.5pt] (3) {$X_4$} {}
							child {node [rectangle,draw] (4) at (1.7,2.8) {$1_2\!\!>\!\!0_2$} edge from parent[draw=none]}
							child {node [rectangle split, rectangle split parts=2,draw] (5) at (1.35,1.8)
								{$1_2\!:\!1_1\!\!>\!\!0_1$ \nodepart{second} $0_2\!:\!0_1\!\!>\!\!1_1$} edge from parent[draw=none]}
							child {node [rectangle,draw] (6) at (0.65,0.8) 
								{$1_2\!:\!1_4\!\!>\!\!0_4$} edge from parent[draw=none]}
					}
	      };
	  \end{tikzpicture}
		\caption{UI-CP PLP-tree \label{fig:UICP_PLPT_compact}}
	\end{subfigure}\\
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level 1/.style={sibling distance=1.5cm, level distance=25pt},
			level 2/.style={sibling distance=0.7cm, level distance=25pt}]
	    \node [main node,inner sep=0.5pt,label={[xshift=0.7cm, yshift=-0.5cm]$\scriptstyle 1_2>0_2$}] (1){$X_2$}
	      child {node [main node,inner sep=0.5pt,label={[xshift=-0.7cm, yshift=-0.5cm]$\scriptstyle 1_3>0_3$}] (3) {$X_3$}
	    		child {node [rectangle,draw] (4) {}}
	    		child {node [rectangle,draw] (5) {}}
				}
	      child {node [main node,inner sep=0.5pt,label={[xshift=0.7cm, yshift=-0.5cm]$\scriptstyle 1_3>0_3$}] (6) {$X_3$}
	      		child {node [rectangle,draw] (8) {}}
	      		child {node [rectangle,draw] (9) {}}
	      };
	  \end{tikzpicture}
		\vspace{-0.1cm}
		\caption{Collapsible PLP-tree \label{fig:UIUP_PLPT_full}}
	\end{subfigure}%
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level/.style={sibling distance=2cm/#1, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$X_2$}
	      child {node [main node,inner sep=0.5pt] (2) {$X_3$}
							child {node [rectangle,draw] (4) at (1.5,1.8) {$1_2\!\!>\!\!0_2$} edge from parent[draw=none]}
							child {node [rectangle,draw] (5) at (0.5,0.9) {$1_3\!\!>\!\!0_3$} edge from parent[draw=none]}
	      };
	  \end{tikzpicture}
		\vspace{-0.2cm}
		\caption{UI-UP PLP-tree \label{fig:UIUP_PLPT_compact}}
	\end{subfigure}\\
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level/.style={sibling distance=2cm/#1, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$X_1$}
	      child {node [main node,inner sep=0.5pt] (2) {$X_2$}
	      	child {node [main node,inner sep=0.5pt] (3) {$X_3$}
							child {node [rectangle,draw] (4) at (1.75,2.7) {$1_1\!\!>\!\!0_1$} edge from parent[draw=none]}
							child {node [rectangle,draw] (5) at (1.3,1.8) {$0_1\!:\!0_2\!\!>\!\!1_2$} edge from parent[draw=none]}
							child {node [rectangle,draw] (6) at (0.8,0.9) {$1_11_2\!:\!1_3\!\!>\!\!0_3$} edge from parent[draw=none]}
					}
	      };
	  \end{tikzpicture}
		\vspace{-0.3cm}
		\caption{Invalid UI-CP PLP-tree \label{fig:invalid_UICP}}
	\end{subfigure}%
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level 1/.style={sibling distance=1.7cm, level distance=25pt},
			level 2/.style={sibling distance=0.7cm, level distance=25pt},
			level 3/.style={sibling distance=0.4cm, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$X_3$}
	      child {node [main node,inner sep=0.5pt] (2) {$X_2$}
	    		child {node [main node,inner sep=0.5pt] (3) {$X_4$}
	      		child {node [rectangle,draw] (4) {}}
	      		child {node [rectangle,draw] (5) {}}
					}
	    		child {node [rectangle,draw] (6) {}}
				}
	      child {node [main node,inner sep=0.5pt] (7) {$X_4$}
	      	child {node [rectangle,draw] (8) {}}
	      	child {node [main node,inner sep=0.5pt] (9) {$X_2$}
	      		child {node [rectangle,draw] (10) {}}
	      		child {node [rectangle,draw] (11) {}}
					}
	      };
	  \end{tikzpicture}
		\caption{CI-FP PLP-tree \label{fig:noncoll_PLPT}}
	\end{subfigure}
  \caption{PLP-trees over the car domain}
  \label{fig:PLPTree}
\end{figure}


\vspace{-0.1cm}
\subsection{Classification of PLP-Trees}

\vspace{-0.1cm}
In the worst case, the size of a PLP-tree is exponential in the number of 
attributes in $\cI$. However, some PLP-trees have a special structure that
allows us to ``collapse'' them and obtain more compact 
representations. This yields a natural classification of PLP-trees, which
we describe below.

%Let $R$ be a sequence of attributes from $\cI$. By $\hat{R}$ we denote the
%set of attributes appearing in $R$. 
Let $R \subseteq \cI$ be the set of attributes that appear in a PLP-tree $T$. We say that $T$ is
\tit{collapsible} if there is a permutation $\hat{R}$ of elements in
$R$ such that for every path in $T$ from the root to a leaf, attributes 
that label nodes on that path appear in the same order in which they 
appear in $\hat{R}$. 

If a PLP-tree $T$ is collapsible, we can represent $T$ by a single path
of nodes labeled with attributes according to the order in which they occur
in $\hat{R}$, 
%(where $\hat{R}$ is the permutation of the set $R$ of attributes in $T$),
where a node labeled with an attribute $X_i$ is also assigned a
\tit{partial conditional preference table} (PCPT) that specifies 
preferences on $X_i$, conditioned on values of ancestor attributes in the path.
These tables make up for the lost structure of $T$ as different ways in 
which ancestor attributes evaluate correspond to different locations 
in the original tree $T$. 
Moreover, missing entries in PCPT of $X_i$ imply equivalence (or indifference) between values of $X_i$
under conditions that do not appear in the PCPT.
Clearly, the PLP-tree in \figref{UICP_PLPT_full} is collapsible, and can
be represented compactly as a single-path tree with nodes labeled by 
attributes in the permutation and PCPTs (cf. \figref{UICP_PLPT_compact}).
Such a collapsed path labeled by attributes is sometimes denoted as
a sequence of attributes in $\hat{R}$ connected by $\triangleright$, e.g.,
$X_2\triangleright X_3\triangleright X_1$ for the path in \figref{UICP_PLPT_compact}.

Collapsible PLP-trees represented by a single path of nodes 
will be referred to as \emph{unconditional importance} trees or $\UI$ 
trees, for short. The name reflects the fact that the order in which 
we consider attributes when seeking the rank of an outcome is always the 
same (not conditioned on the values of ancestor attributes of higher importance).

Let $L$ be a collapsible PLP-tree.
If for every path in $L$ the order of attributes labeling the path is exactly $\hat{R}$,
and $L$ has the same preference $1>0$ on \emph{every} node,
then every PCPT in the collapsed tree contains the same preference $1>0$,
no matter the evaluation of the ancestor attributes.
Thus, every PCPT in the collapsed form can be simplified to a single \emph{fixed} preference $1>0$,
a shorthand for its full-sized counterpart.
We call the resulting
collapsed tree a $\UI$ tree with \emph{fixed preferences}, or a $\UI$-$\FP$ PLP-tree.

A similar simplification is possible if every path in $L$ has the same ordering of attributes
which again is exactly $\hat{R}$, and
for every attribute $X_i$ all nodes in $L$ labeled with $X_i$ have the same
preference on values of $X_i$ (either $1_i>0_i$ or $0_i>1_i$).
Such collapsed 
trees are called $\UI$-$\UP$ PLP-trees, with $\UP$ standing for \emph{unconditional 
preference}. As an example, the $\UI$-$\UP$ tree in \figref{UIUP_PLPT_compact}
is the collapsed representation of the collapsible tree in \figref{UIUP_PLPT_full}.

In all other cases, we refer to collapsed PLP-trees as $\UI$-$\CP$ PLP-trees, 
with $\CP$ standing for \emph{conditional preference}. If preferences on 
an attribute in such a tree depend in an essential way on all preceding 
attributes, there is no real saving in the size of representation (instead 
of an exponential PLP-tree we have a small tree but with preference 
tables that are of exponential size). However, if the preference on an 
attribute depends only on a few higher importance attributes say, never more than 
one or two (or, more generally, never more than some fixed bound $b$), 
the collapsed representation is significantly smaller.

As an aside, we note that not every path of nodes labeled with attributes
and PCPTs is a $\UI$ tree. An example is given in \figref{invalid_UICP}.
Indeed, one can see that there is no PLP-tree that would collapse to 
it. There is a simple condition 
characterizing paths with nodes labeled with attributes and PCPTs that
are valid $\UI$ trees. This matter is not essential to our discussion later 
on and we will not discuss it further here.  
 
When a PLP-tree is not collapsible, the importance of an attribute depends
on where it is located in the tree. We will refer to such PLP-trees as
\tit{conditional importance} trees or $\CI$ trees.

Let $T$ be a $\CI$ PLP-tree.
We call $T$ a $\CI$-$\FP$ tree if every non-leaf node in $T$
is labeled by an attribute with preference $1>0$.
An example of a $\CI$-$\FP$ PLP-tree is shown in \figref{noncoll_PLPT}, where
preferences on each non-leaf node are $1>0$ and hence omitted.
If, for every attribute $X_i$, all nodes in $T$ labeled with $X_i$
have the same preference ($1_i>0_i$ or $0_i>1_i$) on $X_i$, we say $T$ is
a $\CI$-$\UP$ PLP-tree.  
All other non-collapsible PLP-trees are called $\CI$-$\CP$ PLP-trees.
%As mentioned above, the size of $\CI$-$\CP$ trees in the worst case
%could be too large due to the binary tree structure.
%However, if the depth of the tree or the number of paths is bounded 
%by some constant, the size of the model will be substantially reduced.


\section{Passive Learning}

An \tit{example} is a tuple $(\alpha, \beta, v)$, where $\alpha$ and 
$\beta$ are two \emph{distinct} outcomes from combinatorial domain 
$\cX$ over a set $\cI=\{X_1,\ldots,
X_p\}$ of binary attributes, and $v \in \{0,1\}$. An example $(\alpha,\beta,1)$
states that $\alpha$ is strictly preferred to $\beta$ ($\alpha \succ \beta$).
Similarly, an example $(\alpha,\beta,0)$ states that $\alpha$ and $\beta$ 
are equivalent ($\alpha\approx\beta$). Let $\cE=\{e_1,\ldots,e_m\}$ be a set 
of examples over $\cI$, with $e_i=(\alpha_i,\beta_i,v_i)$. We set 
$\cE^{\approx}= \{e_i \in \cE: v_i=0\}$, and $\cE^{\succ}= \{e_i \in \cE: 
v_i =1\}$. 
In the following, we denote by $p$ and $m$ the number of attributes and the number of
examples, respectively.

For a PLP-tree $T$ in full representation we denote by $|T|$ the size of $T$, 
that is, the number of nodes in $T$. If $T$ stands for a $\UI$ tree, we write $|T|$ for
the size of $T$ measured by the total size of preference tables associated
with attributes in $T$. The size of a preference table is the total size of
preferences in it, each preference measured as the number of values in the condition
plus $1$ for the preferred value in the domain of the attribute.
In particular, the sizes of $\UI$-$\FP$ and $\UI$-$\UP$ trees are
given by the number of nodes on the path. 

A PLP-tree $T$ \tit{satisfies} an example $e$ if $T$ orders the two 
outcomes of $e$ in the same way as they are ordered in $e$. Otherwise,
$T$ \tit{falsifies} $e$. Formally, $T$ \tit{satisfies} $e=(\alpha,\beta,1)$
if $\alpha \succ_T \beta$, and $T$ \tit{satisfies} $e=(\alpha,\beta,0)$ if
$\alpha \approx_T \beta$. We say $T$ is \tit{consistent} with a set $\cE$ 
of examples if $T$ satisfies every example in $\cE$.

In this work, we study the following passive learning problems for PLP-trees 
of all types we introduced.
\begin{definition}
Consistent-learning (\tsc{ConsLearn}): given an example set $\cE$, decide 
whether there exists a PLP-tree $T$ (of a particular type) such that $T$ 
is consistent with $\cE$.
\end{definition}

\begin{definition}
Small-learning (\tsc{SmallLearn}): given an example set $\cE$
and a positive integer $l$ ($l \leq |\cE|$), decide whether there 
exists a PLP-tree $T$ (of a particular type) such that $T$ is consistent 
with $\cE$ and $|T| \leq l$.
\end{definition}

\begin{definition}
Maximal-learning (\tsc{MaxLearn}): given an example set $\cE$ and a 
positive integer $k$ ($k \leq m$), decide whether there exists a PLP-tree 
$T$ (of a particular type) such that $T$ satisfies at least $k$ examples 
in $\cE$.
\end{definition}


\section{Learning UI PLP-trees}
In this section, we study the passive learning problems for collapsible 
PLP-trees in their collapsed representations as $\UI$-$\FP$, $\UI$-$\UP$ and
$\UI$-$\CP$ trees.


\vspace{-0.1cm}
\subsection{The \tsc{ConsLearn} Problem}
\vspace{-0.1cm}
%Since how hard is the problem in the setting of $\UI$-$\CP$ trees remains open
%at the time of this paper, we will study it in the future and
%in this work we focus on the other two $\UI$ cases for the \tsc{ConsLearn} problem.

The \tsc{ConsLearn} problem is in the class P for $\UI$-$\FP$ and
$\UI$-$\UP$ trees. To show it, we present a general template of an 
algorithm that learns a $\UI$ tree. Next, for each of the classes 
$\UI$-$\FP$ and $\UI$-$\UP$, we specialize the template to a 
polynomial-time algorithm.

The template algorithm is shown as \algref{learnUI}. The 
input consists of a set $\cE$ of examples and a set $\cI$ of attributes 
from which node labels can be selected. Throughout the execution, the 
algorithm maintains a set $S$ of unused attributes, initialized to $\cI$, 
and a set of examples that are not yet ordered by the tree constructed 
so far.

If the set of strict examples is empty, the algorithm returns an empty tree. 
Otherwise, the algorithm identifies the set $\AI(\cE,S)$ of attributes in 
$S$ that are \emph{available} for selection as the label for the next 
node. If that set is empty, the algorithm terminates with failure. If
not, an attribute, say $X_l$, is selected from $\AI(\cE,S)$, and a PCPT for
that attribute is constructed. Then the sets of examples not ordered yet 
and of attributes not used yet are updated, and the steps repeat.  

\begin{algorithm}[ht]
\KwIn{$\cE$ and $S=\cI$}
\KwOut{A sequence $T$ of attributes from $\cI$ and PCPTs that define
a $\UI$ tree consistent with $\cE$, or FAILURE if such a tree does not 
exist}

	$T \leftarrow \mbox{empty sequence}$\;
	\While{$\cE^\succ \neq \emptyset$}{
        Construct $\AI(\cE,S)$\;
        \If{$\AI(\cE,S)=\emptyset$}{
                \Return FAILURE\;
        }
        $X_l \leftarrow \mbox{an element from $\AI(\cE,S)$}$\;
	Construct $\PCPT(X_l)$\; 
	$T \leftarrow T, (X_l, \PCPT(X_l))$\;
        $\cE \leftarrow \cE \backslash \{e \in \cE^\succ: e$ is decided on $X_l\}$\;
        $S \leftarrow S \backslash \{X_l\}$\;
	}
        \Return $T$\;
\caption{Procedure \tit{learnUI} that learns a UI tree 
\label{alg:learnUI}}
\end{algorithm}

To obtain a learning algorithm for a particular class of $\UI$ trees
($\UI$-$\FP$ or $\UI$-$\UP$) we need to specify the notion 
of an available attribute (needed for line 3) and describe how to construct 
a partial conditional preference table (needed for line 8). 

To this end, let us define $\NEQ(\cE,S)$ to be the set of all attributes in
$S$ (where $S\subseteq \cI$) that incorrectly handle at least one equivalent example 
in $\cE^\approx$. That is, for an attribute $X\in S$ we have $X\in \NEQ(\cE,S)$
precisely when for some example $(\alpha,\beta,0)$ in $\cE$, $\alpha(X)\not=
\beta(X)$. Similarly, let us define $\EQ(\cE,S)$ to be the set of 
attributes in $S$ that do not order any of the strict examples in $\cE$. That 
is, for an attribute $X\in S$ we have $X\in \EQ(\cE,S)$ precisely when 
for every example $(\alpha,\beta,1)$ in $\cE$, $\alpha(X)= \beta(X)$.  

\noindent {\bf Fixed Preferences.}
For the problem of learning $\UI$-$\FP$ trees, we define $\AI(\cE,S)$ to
contain every attribute $X\notin \NEQ(\cE,S)$ such that\\ 
(1) \ for every $(\alpha,\beta,1) \in \cE^\succ$, $\alpha(X) 
\geq \beta(X)$.

\begin{prop}
\label{prop:1}
{\it If there is a $\UI$-$\FP$ tree consistent with all examples in $\cE$ and 
using only attributes from $S$ as labels, then an attribute $X\in S$ is a top node
of some such tree if and only if $X\in \AI(\cE,S)$.}
\end{prop}
\begin{proof}
Let $T$ be a $\UI$ tree consistent with $\cE$ and having only attributes from 
$S$ as labels. Let $X$ be the attribute labeling the top node of $T$. Clearly,
$X\notin \NEQ(\cE,S)$, as otherwise, $T$ would strictly order two outcomes
$\alpha$ and $\beta$ such that $(\alpha,\beta,0)\in \cE^\approx$. To prove
condition (1), let us consider any example $(\alpha,\beta,1)\in \cE^\succ$. 
Since $T$ is consistent with $(\alpha,\beta,1)$, $\alpha(X)\geq\beta(X)$. 
Consequently, $X\in \AI(\cE,S)$.

Conversely, let $X\in \AI(\cE,S)$ and let $T$ be a $\UI$-$\FP$ tree 
consistent with all examples in $\cE$ and using only attributes from $S$ as 
labels (such a tree exists by assumption). If $X$ labels the top node in 
$T$, we are done. Otherwise, let $T'$ be a tree obtained from $T$ by 
adding at the top of $T$ another node, labeling it with $X$ and removing
from $T$ the node labeled by $X$, if such a node exists. By the definition
of $\AI(\cE,S)$ we have that $X\notin\NEQ(\cE,S)$ and that condition (1) 
holds for $X$. Using these properties, we see that $T'$ is also 
a $\UI$-$\FP$ 
tree consistent with all examples in $\cE$. Since the top node of $T'$
is labeled by $X$, the assertion follows. 
\end{proof}

We now specialize \algref{learnUI} by using in line 3 the 
definition of $\AI(\cE,S)$ given above and by setting each $\PCPT(X_l)$ 
to the fixed unconditional preference $1_l>0_l$. Proposition \ref{prop:1}
directly implies the correctness of this version of \algref{learnUI}.

\begin{thm}
Let $\cE$ be a set of examples over a set $\cI$ of binary attributes.
\algref{learnUI} adjusted as described above terminates and outputs
a sequence $T$ representing a $\UI$-$\FP$ tree consistent with $\cE$ if
and only if such a tree exists.
\end{thm}

We note that attributes in $\NEQ(\cE,S)$ are never used when constructing
$\AI(\cE,S)$. Thus, in the case of $\UI$-$\FP$ trees, $S$ could be
initialized to $\cI\setminus\NEQ(\cE,\cI)$. In addition, if an attribute
selected for the label of the top node belongs to $\EQ(\cE^\succ, S)$,
it does not in fact decide any of the strict examples in $\cE$ and can be
dropped. The resulting tree is also consistent with all the examples. 
Thus, the definition of $\AI(\cE,S)$ can be refined by requiring one more
condition: $X \not \in \EQ(\cE^\succ, S)$. That change does not affect 
the correctness of the algorithm but eliminates a possibility of generating
trees with ``redundant'' levels.


\noindent {\bf Unconditional Preferences.}
The case of learning $\UI$-$\UP$ trees is very similar to the 
previous one. Specifically, we define $\AI(\cE,S)$ to contain an attribute 
$X\in S$ precisely when $X\notin\NEQ(\cE,S)$ and

\noindent 
(2) \ for every $(\alpha,\beta,1) \in \cE^\succ$, $\alpha(X) \geq 
\beta(X)$, or for every $(\alpha,\beta,1) \in \cE^\succ$, $\alpha(X) \leq \beta(X)$.

We obtain an algorithm learning $\UI$-$\UP$ trees by using in line 3
the present definition of $\AI(\cE,S)$. In line 8, we take for 
$\PCPT(X_l)$ either $1_l>0_l$ or $0_l>1_l$ (depending on which of the two
cases in (2) holds for $X_l$).

The correctness of this algorithm follows from a property similar to
that in \propref{1}.

As in the previous case, here too $S$ could be initialized to $\cI \setminus
\NEQ$, and the condition $X\not\in \EQ(\cE^\succ,S)$ could be added to the 
definition of $\AI(\cE,S)$. 

\noindent {\bf Conditional Preferences.} 
The problem is in NP because, if a $\UI$-$\CP$
tree consistent with $\cE$ exists (\emph{a priori},
it does not have to have size polynomial in the size of $\cE$), then another
such tree of size polynomial in the size of $\cE$ exists, as well.
We conjecture that the general problem of
learning $\UI$-$\CP$ trees is, in fact, NP-complete. 
As we have only partial results 
for this case, the study of the $\UI$-$\CP$ tree learning will be the subject
of future work. 


\vspace{-0.1cm}
\subsection{The \tsc{SmallLearn} Problem}

\vspace{-0.1cm}
\algref{learnUI} produces a $\UI$ PLP-tree consistent with $\cE$, if one exists.
In many cases, it is desirable to compute a small, sometimes even the smallest, 
representation consistent with $\cE$.
We show that these problems for $\UI$ trees are NP-hard.

\begin{thm}
\label{thm:UIFP_smallest_decision}
	The \tsc{SmallLearn} problem is NP-complete for each class of $\{\UI\} \times \{\FP,\UP,\CP\}$.
\end{thm}
\begin{proof}
	We present the proof only in the case of $\UI$-$\FP$. The argument in other cases 
	($\UI$-$\UP$ and $\UI$-$\CP$) is similar.

	(Membership) One can guess a $\UI$-$\FP$ PLP-tree $T$ in linear time, and verify in polynomial time that 
	$T$ has at most $l$ attributes and satisfies every example in $\cE$.

	(Hardness) We present a polynomial-time reduction from the
	\tit{hitting set problem} (HSP), which is NP-complete 
    \cite{Garey:1979}. To recall, in HSP we are given a finite
		set $U=\{a_1,\ldots,a_n\}$, a collection $C=\{S_1,\ldots,
    S_d\}$ of subsets of $U$ with $\bigcup_{S_i \in C} S_i = U$,
    and a positive integer $k \leq n$, and the problem is to
    decide whether $U$ has a hitting set $U'$ such that $|U'|\leq k$ 
    ($U'\subseteq U$ is a \emph{hitting} set for $C$ if 
		$U' \cap S_i \not = \emptyset$ for all $S_i \in C$).
	Given an instance of HSP, we construct an instance of our problem as follows.

	\noindent 1. $\cI=\{X_i: a_i \in U\}$ (thus, $p=n$).

	\noindent 2. $\cE=\{(\tbf{s}_i, \tbf{0},1): S_i \in C\}$, where
	$\tbf{s}_i$ is a $p$-bit vector such that $\tbf{s}_i[j]=1 \Leftrightarrow a_j \in S_i$
	and $\tbf{s}_i[j]=0 \Leftrightarrow a_j \not \in S_i$ ($1 \leq j \leq p$), 
	and $\tbf{0}$ is a $p$-bit vector of all $0$'s (thus, $m=d$).

	\noindent 3. We set $l=k$.

	We need to show that $U$ has a hitting set of size at most $k$ if and only if
	there exists a $\UI$-$\FP$ PLP-tree of size at most $l$ consistent with $\cE$.

\smallskip
\noindent
	($\Rightarrow$) Assume $U$ has a hitting set $U'$ of size $k$.
	Let $U'$ be $\{a_{j_1},\ldots,a_{j_k}\}$.
	Define a $\UI$-$\FP$ PLP-tree $L=X_{j_1} \triangleright \ldots \triangleright X_{j_k}$.
	We show that $L$ is consistent with $\cE$.
	Let $e=(\alpha_e,\beta_e,1)$ be an arbitrary example in $\cE$, where
	$\alpha_e=\tbf{s}_i$ and $\beta_e=\tbf{0}$.
	Since $U'$ is a hitting set, %for every set $S_i\in C$ 
        there exists $r$, $1 \leq r \leq k$, 
	%$a_{j_r}$ ($1 \leq r \leq k$) 
        such that $a_{j_r} \in S_i$.
	Thus, %for $\alpha_e$ 
        there exists $r$, $1 \leq r \leq k$, %$X_{j_r}$ ($1 \leq r \leq k$)
	such that $\alpha_e(X_{j_r})=1$.
	Let $r$ be the smallest with this property. 
	%Among these attributes $X_{j_r}$ let $X_j$ be the top-ranked one.
	It is clear that $e$ is decided at $X_{j_r}$; thus, we have 
	$\alpha_e \succ_L \beta_e$.

\smallskip
\noindent
	($\Leftarrow$) Assume there is a $\UI$-$\FP$ PLP-tree $L$ of $l$ attributes in
	$\cI$ such that $L$ is consistent with $\cE$. Moreover, we assume
	$L=X_{j_1} \triangleright \ldots \triangleright X_{j_l}$.
	Let $U'=\{a_{j_1}, \ldots,a_{j_l}\}$.
	We show by means of contradiction.  Assume that $U'$ is not a hitting set.
	That is, there exists a set $S_i \in C$ such that
	$U' \cap S_i = \emptyset$.
	Then, there exists an example $e=(\alpha_e,\beta_e,1)$, where $\alpha_e=\tbf{s}_i$ 
	and $\beta_e=\tbf{0}$, such that $\alpha_e \approx_L \beta_e$ because
	none of the attributes $\{X_i:\alpha_e(X_i)=1\}$ show up in $L$. This is
	a contradiction! Thus, $U'$ is a hitting set.
\end{proof}

\begin{cor}
\label{cor:UIFP_smallest}
	Given a set $\cE$ of examples $\{e_1,\ldots,e_m\}$ over $\cI=\{X_1,\ldots,X_p\}$,
	finding the smallest PLP-tree in each class of $\{\UI\} \times \{\FP,\UP,\CP\}$ 
	consistent with $\cE$ is NP-hard.
\end{cor}

Consequently, it is important to study fast heuristics that aim at
approximating trees of optimal size. 
Here, we propose a greedy heuristic for \algref{learnUI}.
In every iteration the heuristic selects the attribute $X_l \in \AI(\cE,S)$ that
decides the most examples in $\cE^\succ$.
However, for some dataset the resulting greedy algorithm does not perform 
well: the ratio of the size of the tree computed by our algorithm 
to the size of the optimal sequence may be as large as $\Omega(p)$.
To see this, we consider the following input.

\begin{framed}
	\vspace{-0.2cm}
	\noindent $(1_10_20_30_4,0_10_20_30_4,1)$\\
	$(1_11_20_30_4,0_10_20_30_4,1)$\\
	$(1_10_21_30_4,0_10_20_30_4,1)$\\
	$(0_10_20_31_4,1_10_20_30_4,1)$
	\vspace{-0.2cm}
\end{framed}

For each class of $\{\UI\} \times \{\FP,\UP\}$, \algref{learnUI} in the worst 
case computes $X_2 \triangleright X_3 \triangleright X_4 \triangleright X_1$,
whereas the optimal tree is $X_4 \triangleright X_1$ (with
the PCPTs omitted as they contain only one preference and so, they do not
change the asymptotic size of the tree). This example generalizes to the
arbitrary number $p$ of attributes. Thus, the greedy algorithm to learn small
$\UI$ trees is no better than any other algorithm in 
the worst case.

Approximating HSP has been extensively studied over the last decades.
It has been shown \cite{lund1994hardness} that, unless $\NP \subset \DTIME(n^{\poly \log n})$,
HSP cannot be approximated 
in polynomial time within factor of $c \log n$, where $0<c<\frac{1}{4}$ and
$n$ is the number of elements in the input. The reduction we used above
shows that this result
carries over to our problem.
\begin{thm}
\label{thm:UI_smallest_approx}
	Unless $\NP \subset \DTIME(n^{\poly \log n})$,
	the problem of finding the smallest PLP-tree in each class of $\{\UI\} \times \{\FP,\UP,\CP\}$ 
	consistent with $\cE$ cannot be approximated 
	in polynomial time within factor of $c \log p$, where $0<c<\frac{1}{4}$.
\end{thm}
It is an open problem whether this result can be strengthened to a factor 
linear in $p$ (cf. the example for the worst-case behavior of our simple 
greedy heuristic).


\vspace{-0.1cm}
\subsection{The \tsc{MaxLearn} Problem}

\vspace{-0.1cm}
When there is no $\UI$ PLP-tree consistent with the set of all examples,
it may be useful to learn a $\UI$ PLP-tree satisfying as many examples 
as possible. We show this problem is in fact NP-hard for all three 
classes of $\UI$ trees.

\begin{thm}
\label{thm:UIFP_least_decision}
The \tsc{MaxLearn} problem is NP-complete for each class of $\{\UI\} \times \{\FP,\UP,\CP\}$.
\end{thm}
\begin{proof}
The problem is in NP. This is evident for the case
of $\UI$-$\FP$ and $\UI$-$\UP$ trees. If $\cE$ is a given set of examples,
and $k$ a required lower bound on the number of examples that are to be 
correctly ordered, then witness trees in these classes (trees that correctly 
order at least $k$ examples in $\cE$) have size polynomial in the size of 
$\cE$. Thus, verification can be performed in polynomial time. 
For the case of $\UI$-$\CP$ trees, if there is a $\UI$-$\CP$
tree correctly ordering at least $k$ examples in $\cE$, then there exists
such tree of size polynomial in $|\cE|$.

The hardness part follows from the proof in the setting of learning 
lexicographic strategies \cite{schmitt2006complexity}, adapted to the case 
of $\UI$ PLP-trees.
\end{proof}

\begin{cor}
\label{cor:UIFP_least}
Given a set $\cE$ of examples $\{e_1,\ldots,e_m\}$ over 
$\cI=\{X_1,\ldots,X_p\}$, finding a PLP-tree in each class of $\{\UI\} \times \{\FP,\UP,\CP\}$ 
satisfying the maximum number of examples in $\cE$ is NP-hard.
\end{cor}

\section{Learning CI PLP-trees}
Finally, we present results on the passive learning problems for 
PLP-trees in classes $\{\CI\} \times \{\FP,\UP,\CP\}$. We recall that these
trees assume full (non-collapsed) representation.


\vspace{-0.1cm}
\subsection{The \tsc{ConsLearn} Problem}

\vspace{-0.1cm}
We first show that the \tsc{ConsLearn} problem for class $\CI$-$\UP$ is 
NP-complete. We then propose polynomial-time algorithms to solve the 
\tsc{ConsLearn} problem
%so as to show that the problem is in class P, 
for the classes $\CI$-$\FP$ and $\CI$-$\CP$.

\begin{thm}
\label{thm:passlearn_CIUP}
	The \tsc{ConsLearn} problem is NP-complete for class $\CI$-$\UP$.
\end{thm}
\begin{proof}
The problem is in NP because the size of a witness, 
a $\CI$-$\UP$ PLP-tree consistent with $\cE$, is bounded by $|\cE|$ (
if a $\CI$-$\UP$ tree consistent with $\cE$ exists, then it can
be modified to a tree of size no larger than $O(|\cE|)$).
	Hardness follows from the proof by Booth et al. \shortcite{booth:learningLP} showing 
	\tsc{ConsLearn} is NP-hard in the setting of LP-trees.
\end{proof}

For the two other classes of trees, the problem is in P. This is demonstrated 
by polynomial-time \algref{recur_learnCI} adjusted for both classes.

\begin{algorithm}[ht]
\KwIn{$\cE$, $S=\cI$, and $t$: an unlabeled node}
\KwOut{A CI PLP-tree over $S$ consistent with $\cE$, or FAILURE}
	\If{$\cE^\succ = \emptyset$}{
		Label $t$ as a leaf and \Return\;
	}
	Construct $\AI(\cE,S)$\;
	\If{$\AI(\cE,S)=\emptyset$}{
		\Return FAILURE and terminate\;
	}
	Label $t$ with tuple $(X_l,x_l)$ where $X_l$ is from $\AI(\cE,S)$, 
		and $x_l$ is the preferred value on $X_l$\;
	$\cE \leftarrow \cE \backslash \{e \in \cE^\succ: e$ is decided on $X_l\}$\;
	$S \leftarrow S \backslash \{X_l\}$\;
	Create two edges $u_l,u_r$ and two unlabeled nodes $t_l,t_r$ such that $u_l=\langle t,t_l\rangle$
		and $u_r=\langle t,t_r\rangle$\;
	$\cE_l \leftarrow \{e\in \cE: \alpha_e(X_j)=\beta_e(X_j)=x_l\}$\;
	$\cE_r \leftarrow \{e\in \cE: \alpha_e(X_j)=\beta_e(X_j)=\overline{x_l}\}$\;
	$\mathit{learnCI}(\cE_l,S,t_l)$\;
	$\mathit{learnCI}(\cE_r,S,t_r)$\;

\caption{The recursive procedure \tit{learnCI} that learns a CI PLP-tree \label{alg:recur_learnCI}}
\end{algorithm}

\noindent {\bf Fixed Preference.}
For class $\CI$-$\FP$, we define $\AI(\cE,S)$ to contain attribute 
$X\notin \NEQ(\cE,S)$ if\\
(3) \ for every $(\alpha,\beta,1) \in \cE^\succ$,  $\alpha(X) \geq \beta(X)$.

\begin{prop}
\label{prop:2}
{\it If there is a $\CI$-$\FP$ tree consistent with all examples in $\cE$ and 
using only attributes from $S$ as labels, then an attribute $X\in S$ is a top node
of some such tree if and only if $X\in \AI(\cE,S)$.}
\end{prop}
\begin{proof}
	It is clear that if there exists a $\CI$-$\FP$ PLP-tree consistent with $\cE$
	and only using attributes from $S$ as labels, then the fact that
	$X \in S$ labels the root of some such tree implies $X \in \AI(\cE,S)$.

	Now we show the other direction.
	Let $T$ be the $\CI$-$\FP$ tree over a subset of $S$ consistent 
	with $\cE$,
	$X$ be an attribute such that $X \in \AI(\cE,S)$.
	If $X$ is the root attribute in $T$, we are done.
	Otherwise, we construct a $\CI$-$\FP$ tree $T'$ by creating a root, 
	labeling it with $X$, and make one copy of $T$ the left subtree of $T'$
        ($T_l'$) and another, the right subtree of $T'$ ($T_r'$).
	For a node $t$ and a subtree $B$ in $T$, we write $t_l'$ and $B_l'$, 
	respectively, for the corresponding node and subtree in $T_l'$.
	We define $t_r'$ and $B_r'$ similarly. If $X$ does not appear in 
	$T$, we are done constructing $T'$; otherwise, we update $T'$ as 
	follows.

	\noindent 1). For every node $t\in T$ labeled by $X$ such that $t$ has two leaf children,
	we replace the subtrees rooted at $t_l'$ and $t_r'$ in $T_l'$ and $T_r'$ with leaves.

	\noindent 2). For every node $t \in T$ labeled by $X$ such that $t$ has one leaf child
	and a non-leaf subtree $B$, we replace the subtree rooted at $t_l'$ in $T_l'$ with $B_l'$,
	and the subtree rooted at $t_r'$ in $T_r'$ with a leaf,
	if $t \in T$ has a right leaf child; otherwise, we replace the subtree rooted at
	$t_l'$ in $T_l'$ with a leaf, and the subtree rooted at $t_r'$ in $T_r'$ with $B_r'$.

	\noindent 3). Every other node $t \in T$ labeled by $X$ has two non-leaf subtrees:
	left non-leaf subtree $\BL$ and right $\BR$.
	For every such node $t \in T$, we replace the subtree rooted at $t_l'$ in $T_l'$ with
	$\BL_l'$, and the subtree rooted at $t_r'$ in $T_r'$ with $\BR_r'$.

	As an example, this construction of $T'$ from $T$ is demonstrated in \figref{promote}.
	We see that this construction results in a $\CI$-$\CP$ tree consistent
	with $\cE$ and, clearly, it has its root labeled with $X$.
	Thus, the assertion follows.
\end{proof}

\begin{figure}
	\centering
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
		    level 1/.style={sibling distance=2.8cm, level distance=25pt},
		    level 2/.style={sibling distance=1.3cm, level distance=25pt},
				level 3/.style={sibling distance=0.8cm, level distance=25pt},
				level 4/.style={sibling distance=0.5cm, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$\redtext{X_1}$}
	      child {node [main node,inner sep=0.5pt] (2) {$\bluetext{X_2}$}
	      	child {node [main node,inner sep=0.5pt] (3) {$X_3$} {}
		      	child {node [main node,inner sep=0.5pt] (4) {$X_4$}
		      		child {node [rectangle,draw] (5) {}}
		      		child {node [rectangle,draw] (6) {}}
						}
		      	child {node [rectangle,draw] (7) {}}
					}
	      	child {node [main node,inner sep=0.5pt] (8) {$X_4$} {}
		      	child {node [rectangle,draw] (9) {}}
		      	child {node [main node,inner sep=0.5pt] (10) {$X_3$}
		      		child {node [rectangle,draw] (11) {}}
		      		child {node [rectangle,draw] (12) {}}
						}
					}
	      }
	      child {node [main node,inner sep=0.5pt] (13) {$X_3$}
	      	child {node [main node,inner sep=0.5pt] (14) {$\bluetext{X_2}$} {}
		      	child {node [rectangle,draw] (15) {}}
		      	child {node [rectangle,draw] (16) {}}
					}
	      	child {node [main node,inner sep=0.5pt] (17) {$\bluetext{X_2}$} {}
		      	child {node [main node,inner sep=0.5pt] (18) {$X_4$}
		      		child {node [rectangle,draw] (19) {}}
		      		child {node [rectangle,draw] (20) {}}
						}
		      	child {node [rectangle,draw] (21) {}}
					}
				};
	  \end{tikzpicture}
		\caption{$T$ \label{fig:promote_T}}
	\end{subfigure}\\
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
		    level 1/.style={sibling distance=2.8cm, level distance=25pt},
		    level 2/.style={sibling distance=1.3cm, level distance=25pt},
				level 3/.style={sibling distance=0.8cm, level distance=25pt},
				level 4/.style={sibling distance=0.5cm, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$\bluetext{X_2}$}
	      child {node [main node,inner sep=0.5pt] (2) {$\redtext{X_1}$}
	      	child {node [main node,inner sep=0.5pt] (3) {$X_3$} {}
		      	child {node [main node,inner sep=0.5pt] (4) {$X_4$}
		      		child {node [rectangle,draw] (5) {}}
		      		child {node [rectangle,draw] (6) {}}
						}
		      	child {node [rectangle,draw] (7) {}}
					}
	      	child {node [main node,inner sep=0.5pt] (8) {$X_3$} {}
		      	child {node [rectangle,draw] (9) {}}
		      	child {node [main node,inner sep=0.5pt] (10) {$X_4$}
		      		child {node [rectangle,draw] (11) {}}
		      		child {node [rectangle,draw] (12) {}}
						}
					}
	      }
	      child {node [main node,inner sep=0.5pt] (13) {$\redtext{X_1}$}
	      	child {node [main node,inner sep=0.5pt] (14) {$X_4$} {}
		      	child {node [rectangle,draw] (15) {}}
		      	child {node [main node,inner sep=0.5pt] (16) {$X_3$}
		      		child {node [rectangle,draw] (17) {}}
		      		child {node [rectangle,draw] (18) {}}
						}
					}
	      	child {node [main node,inner sep=0.5pt] (19) {$X_3$} {}
		      	child {node [rectangle,draw] (20) {}}
		      	child {node [rectangle,draw] (21) {}}
					}
				};
	  \end{tikzpicture}
		\caption{$T'$ \label{fig:promote_T_updated}}
	\end{subfigure}
	\caption{$X_2 \in \AI(\cE,S)$ is picked at the root \label{fig:promote}}
\end{figure}

\propref{2} clearly implies the correctness of \algref{recur_learnCI} with 
$\AI(\cE,S)$ defined as above for class $\CI$-$\FP$ and each $x_l \in 
(X_l,x_l)$ set to $1$.

\begin{thm}
Let $\cE$ be a set of examples over a set $\cI$ of binary attributes.
\algref{recur_learnCI} adjusted as described above terminates and outputs
a $\CI$-$\FP$ tree $T$ consistent with $\cE$ if
and only if such a tree exists.
\end{thm}


\noindent {\bf Conditional Preference.}
For class $\CI$-$\CP$, we define that $\AI(\cE,S)$ contains attribute $X \not \in \NEQ(\cE)$ if 

\noindent (4) \ for every $(\alpha,\beta,1) \in \cE^\succ$, $\alpha(X) \geq 
\beta(X)$, or for every $(\alpha,\beta,1) \in \cE^\succ$, $\alpha(X) \leq \beta(X)$.

We obtain an algorithm learning $\CI$-$\CP$ trees by using in line 4
the present definition of $\AI(\cE,S)$. In line 8, we take for 
$x_l$ either $1$ or $0$ (depending on which of the two
cases in (4) holds for $X_l$).
The correctness of this algorithm follows from a property similar to
that in \propref{2}.


\vspace{-0.1cm}
\subsection{The \tsc{SmallLearn} and \tsc{MaxLearn} Problems}

\vspace{-0.1cm}
We outline the results we have for this case. 
Both problems for the three $\CI$ classes are NP-complete. They are in 
NP since if a witness PLP-tree exists, one can modify it so that 
its size does not exceed the size of the input. Hardness of the 
\tsc{SmallLearn} problem for $\CI$ classes follows from the proof of 
\thmref{UIFP_smallest_decision}, whereas the hardness of the 
\tsc{MaxLearn} problem for $\CI$ cases follows from the proof 
by Schmitt and Martignon \shortcite{schmitt2006complexity}.


%\section{Possible Experiment Settings}
%\begin{enumerate}
%	\item Generate a random PLP-tree $T_r$, generate a set of examples $\cE$ consistent with $T_r$;
%				learn a PLP-tree $T_l$ consistent with $\cE$;
%				compare $T_r$ and $T_l$ (e.g., compute the percentage of agreed and disagreed pairs
%				\cite{conf/adt13/JG}).
%	\item Generate random consistent examples, learn a PLP-tree and predict preference on new examples
%				based on the tree learned, and compare the results against machine learning 
%				results \cite{busa2014pac}.
%	\item Generate random examples, learn a PLP-tree that satisfies many examples 
%				and predict preference on new examples
%				based on the tree learned, and compare the results against machine learning 
%				results \cite{busa2014pac}.
%\end{enumerate}


\section{Conclusions}
We proposed a preference language, \tit{partial lexicographic preference 
trees}, \tit{PLP-trees}, as a way to represent preferences over combinatorial 
domains. For several natural classes of PLP-trees, we studied passive learning 
problems: \tsc{ConsLearn}, \tsc{SmallLearn} and \tsc{MaxLearn}. All complexity
results we obtained are summarized in tables in \tblref{comp_results}. The
\tsc{ConsLearn} problem for $\UI$-$\CP$ trees is as of now unsettled. While we are aware 
of subclasses of $\UI$-$\CP$ trees for which polynomial-time algorithms are 
possible, we conjecture that in general, the problem is NP-complete.

\begin{table}[!ht]
	\centering
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
		\begin{tabular}[0.45\textwidth]{ | c | c | c | c | }
		  \hline
		     & FP & UP & CP \\
		  \hline
		  UI & P & P & \tit{NP}\\
		  \hline
		  CI & P & NPC & P  \\
		  \hline
		\end{tabular}
		\caption{\tsc{ConsLearn}}
		\label{tbl:cons_learn}
	\end{subfigure}%
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
		\begin{tabular}[0.45\textwidth]{ | c | c | c | c | }
		  \hline
		     & FP & UP & CP \\
		  \hline
		  UI & NPC & NPC & NPC \\
		  \hline
		  CI & NPC & NPC & NPC \\
		  \hline
		\end{tabular}
		\caption{\tsc{SmallLearn} \& \tsc{MaxLearn}}
		\label{tbl:small_max_learn}
	\end{subfigure}
	\caption{Complexity results for passive learning problems}
	\label{tbl:comp_results}
\end{table}

For the future research, we will develop good heuristics for our learning algorithms.
We will implement these algorithms handling attributes of, in general, finite domains
of values, and evaluate them on both synthetic and
real-world preferential datasets. 
With PLP-trees of various classes learned, we will compare our models with
the ones learned through other learning approaches on predicting new preferences.
