\paragraph{\bf Abstract.}
We introduce \tit{partial lexicographic preference trees} (PLP-trees) 
as a formalism for compact representations of preferences over 
combinatorial domains. Our main results concern the problem of passive 
learning of PLP-trees. Specifically, for several classes of 
PLP-trees, we study how to learn (i) a PLP-tree consistent with a 
dataset of examples, possibly subject to requirements on the size 
of the tree, and (ii) a PLP-tree correctly ordering as many of 
the examples as possible in case the dataset of examples is inconsistent. 
We establish complexity of these problems and, in all cases where the 
problem is in the class P, propose polynomial time algorithms.

\section{Introduction}
Representing and reasoning about preferences are fundamental to decision
making and so, of significant interest to artificial intelligence. When 
the choice is among a few \emph{outcomes} (\emph{alternatives}),
representations in terms of explicit enumerations of the preference order 
are feasible and lend themselves well to formal analysis. However, in many
applications outcomes come from \emph{combinatorial domains}. That is,
outcomes are described as tuples of values of \emph{issues} (also referred 
to as \emph{variables} or \emph{attributes}), say $X_1,\ldots, X_p$, with 
each issue $X_i$ assuming values from some set $D_i$ --- its \emph{domain}. 
Because of the combinatorial size of the space of such outcomes, explicit
enumerations of their elements are impractical. Instead, we resort to 
formalisms supporting intuitive and, ideally, concise \emph{implicit}
descriptions of the order. The language of CP-nets \cite{bbdh03} is a prime
example of such a formalism. 

Recently, however, there has been a rising interest in representing
preferences over combinatorial domains by exploiting the notion of
the lexicographic ordering. For instance, assuming issues are over the  
binary domain $\{0,1\}$, with the preferred value for each issue being 
1, a sequence of issues naturally determines an order on outcomes.
This idea gave rise to the language of \tit{lexicographic preference 
models} or \tit{lexicographic strategies}, which has been extensively 
studied in the literature \cite{schmitt2006complexity,dombi2007learning,%
yaman2008democratic,Wilson14}. The formalism of complete \tit{lexicographic 
preference trees} (LP-trees) \cite{booth:learningLP} generalizes the
language of lexicographic strategies by arranging issues into 
decision trees that assign preference ranks to outcomes. An important
aspect of LP-trees is that they allow us to model \emph{conditional}
preferences on issues and \emph{conditional} ordering of issues. 
Another formalism,
the language of \tit{conditional lexicographic preference trees} (or 
CLP-trees) \cite{brauning2012learning}, extends LP-trees by allowing 
subsets of issues as labels of nodes.

A central problem in preference representation concerns learning 
implicit models of preferences (such as lexicographic strategies,
LP-trees or CLP-trees), of possibly small sizes, that are consistent 
with all (or at least possibly many) given examples, each correctly 
ordering a pair of outcomes. The problem was extensively studied. Booth
et al. \shortcite{booth:learningLP} considered learning of LP-trees, and 
Br\"auning and Eyke \shortcite{brauning2012learning} of CLP-trees. 

In this paper, we introduce \emph{partial lexicographic preference trees} (or 
\emph{PLP-trees}) as means to represent \emph{total preorders} over 
combinatorial domains. PLP-trees are closely related to LP-trees
requiring that every path in the tree contains all issues 
used to describe outcomes. Consequently, LP-trees describe total 
orders over the outcomes. PLP-trees relax this requirement and allow 
paths on which some issues may be missing. Hence, 
PLP-trees describe total preorders. This seemingly small difference
has a significant impact on some of the learning problems. It allows
us to seek PLP-trees that minimize the set of issues on their paths,
which may lead to more robust models by disregarding issues 
that have no or little influence on the true preference (pre)order.

The rest of the paper is organized as follows. In the next section, 
we introduce the language of PLP-trees and describe a classification 
of PLP-trees according to their complexity. We also define three types of passive 
learning problems for the setting of PLP-trees. In the following 
sections, we present algorithms learning PLP-trees of particular
types and computational complexity results on the existence of 
PLP-trees of different types, given size or accuracy. We close 
with conclusions and a brief account of future work.


\section{Partial Lexicographic Preference Trees}
Let $\cI=\{X_1,\ldots,X_p\}$ be a set of binary issues, with each
$X_i$ having its domain $D_i=\{0_i, 1_i\}$. The corresponding 
\tit{combinatorial domain} is the set $\cX=D_1 \times \ldots \times D_p$.
Elements of  $\cX$ are called \tit{outcomes}.

A PLP-tree over $\cX$ is binary tree whose every
non-leaf node is labeled by an issue from $\cI$ and by a preference
entry $1>0$ or $0>1$, and whose every leaf node is denoted by a box 
$\Box$. Moreover, we require that on every path from the root to a leaf
each issue appears \emph{at most} once. 

To specify the total preorder on outcomes defined by a PLP-tree $T$, let 
us enumerate leaves of $T$ from left to right, assigning them 
integers $1,2$, etc. For every outcome $\alpha$ we find its leaf 
in $T$ by starting at the root of $T$ and proceeding downward.
When at a node labeled with an issue $X$, we descend to the left or to the 
right child of that node based on the value $\alpha(X)$ of the issue $X$ 
in $\alpha$ and on the preference assigned to that node. If $\alpha(X)$ is 
the preferred value, we descend to the left child. We descend to the right
child,
otherwise. The integer assigned to the leaf that we eventually get 
to is the \emph{rank} 
of $\alpha$ in $T$, written $r_T(\alpha)$. The preorder $\succeq_T$ on 
distinct outcomes determined by $T$ is defined as follows: $\alpha\succeq_T \beta$ 
if $r_T(\alpha)\leq r_T(\beta)$ (smaller ranks are ``better''). We also 
define derived relations $\succ_T$ (strict order) and $\approx_T$ 
(equivalence or indifference): $\alpha \succ_T\beta$ if $\alpha\succeq_T
\beta$ and $\beta \not\succeq_T \alpha$, and $\alpha_T\approx_T\beta$ if 
$\alpha\succeq_T\beta$ and $\beta\succeq_T \alpha$. Clearly, $\succeq_T$ 
is a total preorder on outcomes partitioning them into strictly ordered 
clusters of equivalent outcomes. 
%To simplify the notation, we always drop
%the sbscript $T$  whenever it is clear from the context.

To illustrate the notions just introduced, we consider preference orderings 
of dinner options over four binary issues. The \tit{appetizer} ($X_1$) can 
be either \tit{salad} ($0_1$) or \tit{soup} ($1_1$). The \tit{main course} 
($X_2$)
is either \tit{beef} ($0_2$) or \tit{fish} ($1_2$).
The \tit{drink} ($X_3$) can be \tit{beer} ($0_3$) or (white) \tit{wine} 
($1_3$). Finally, \tit{dessert} ($X_4$) can be \tit{ice-cream} ($0_4$)
or \tit{pie} ($1_4$).
An agent could specify her preferences over dinners as a PLP-tree in 
\figref{UICP_PLPT_full}. The \emph{main course} is the most important issue to 
the agent and she prefers fish to beef. Her next most important issue is 
what to \emph{drink} (independently of her selection for the main course). She 
prefers wine over beer with fish, and beer over wine with beef. If the
agent has beef for her main dish, no matter what drink she gets, her next 
consideration is the \emph{appetizer}, and she prefers salad to soup.
In this example, the \emph{dessert} does not figure into preferences at all.
The most preferred dinners have fish for the main course and wine for the 
drink, with all possible combinations of choices for the appetizer and 
dessert (and so, the cluster of most preferred dinners has four elements).

\begin{figure}[!ht]
	\centering
  \begin{subfigure}[b]{0.23\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level 1/.style={sibling distance=2cm, level distance=25pt},
			level 2/.style={sibling distance=0.7cm, level distance=25pt},
			level 3/.style={sibling distance=0.4cm, level distance=25pt}]
	    \node [main node,inner sep=0.5pt,label={[xshift=0.7cm, yshift=-0.5cm]$\scriptstyle 1_2>0_2$}] (1){$X_2$}
	      child {node [main node,inner sep=0.5pt,label={[xshift=-0.7cm, yshift=-0.5cm]$\scriptstyle 1_3>0_3$}] (3) {$X_3$}
	    		child {node [rectangle,draw] (4) {}}
	    		child {node [rectangle,draw] (5) {}}
				}
	      child {node [main node,inner sep=0.5pt,label={[xshift=0.7cm, yshift=-0.5cm]$\scriptstyle 0_3>1_3$}] (6) {$X_3$}
	      	child {node [main node,inner sep=0.5pt,label={[xshift=-0.65cm, yshift=-0.5cm]$\scriptstyle 0_1>1_1$}] (7) {$X_1$}
	      		child {node [rectangle,draw] (8) {}}
	      		child {node [rectangle,draw] (9) {}}
					}
	      	child {node [main node,inner sep=0.5pt,label={[xshift=0.7cm, yshift=-0.5cm]$\scriptstyle 0_1>1_1$}] (10) {$X_1$}
	      		child {node [rectangle,draw] (11) {}}
	      		child {node [rectangle,draw] (12) {}}
					}
	      };
	  \end{tikzpicture}
		\caption{Collapsible PLP-tree \label{fig:UICP_PLPT_full}}
	\end{subfigure}%
  \begin{subfigure}[b]{0.23\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level/.style={sibling distance=2cm/#1, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$X_2$}
	      child {node [main node,inner sep=0.5pt] (2) {$X_3$}
	      	child {node [main node,inner sep=0.5pt] (3) {$X_1$} {}
							child {node [rectangle,draw] (4) at (1.75,2.7) {$\top\!:\!1_2\!\!>\!\!0_2$} edge from parent[draw=none]}
							child {node [rectangle split, rectangle split parts=2,draw] (5) at (1.2,1.8)
								{$1_2\!:\!1_3\!\!>\!\!0_3$ \nodepart{second} $0_2\!:\!0_3\!\!>\!\!1_3$} edge from parent[draw=none]}
							child {node [rectangle,draw] (6) at (0.5,0.85) 
								{$0_2\!:\!0_1\!\!>\!\!1_1$} edge from parent[draw=none]}
					}
	      };
	  \end{tikzpicture}
		\caption{UI-CP PLP-tree \label{fig:UICP_PLPT_compact}}
	\end{subfigure}\\
  \begin{subfigure}[b]{0.23\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level 1/.style={sibling distance=1.5cm, level distance=25pt},
			level 2/.style={sibling distance=0.7cm, level distance=25pt}]
	    \node [main node,inner sep=0.5pt,label={[xshift=0.7cm, yshift=-0.5cm]$\scriptstyle 1_2>0_2$}] (1){$X_2$}
	      child {node [main node,inner sep=0.5pt,label={[xshift=-0.7cm, yshift=-0.5cm]$\scriptstyle 0_3>1_3$}] (3) {$X_3$}
	    		child {node [rectangle,draw] (4) {}}
	    		child {node [rectangle,draw] (5) {}}
				}
	      child {node [main node,inner sep=0.5pt,label={[xshift=0.7cm, yshift=-0.5cm]$\scriptstyle 0_3>1_3$}] (6) {$X_3$}
	      		child {node [rectangle,draw] (8) {}}
	      		child {node [rectangle,draw] (9) {}}
	      };
	  \end{tikzpicture}
		\vspace{-0.1cm}
		\caption{Collapsible PLP-tree \label{fig:UIUP_PLPT_full}}
	\end{subfigure}%
  \begin{subfigure}[b]{0.23\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level/.style={sibling distance=2cm/#1, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$X_2$}
	      child {node [main node,inner sep=0.5pt] (2) {$X_3$}
							child {node [rectangle,draw] (4) at (1.6,1.8) {$\top\!:\!1_2\!\!>\!\!0_2$} edge from parent[draw=none]}
							child {node [rectangle,draw] (5) at (0.6,0.9) {$\top\!:\!0_3\!\!>\!\!1_3$} edge from parent[draw=none]}
	      };
	  \end{tikzpicture}
		\vspace{-0.2cm}
		\caption{UI-UP PLP-tree \label{fig:UIUP_PLPT_compact}}
	\end{subfigure}\\
  \begin{subfigure}[b]{0.23\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level/.style={sibling distance=2cm/#1, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$X_1$}
	      child {node [main node,inner sep=0.5pt] (2) {$X_2$}
	      	child {node [main node,inner sep=0.5pt] (3) {$X_3$}
							child {node [rectangle,draw] (4) at (1.75,2.7) {$\top\!:\!1_1\!\!>\!\!0_1$} edge from parent[draw=none]}
							child {node [rectangle,draw] (5) at (1.1,1.8) {$0_1\!:\!0_2\!\!>\!\!1_2$} edge from parent[draw=none]}
							child {node [rectangle,draw] (6) at (0.6,0.9) {$1_11_2\!:\!1_3\!\!>\!\!0_3$} edge from parent[draw=none]}
					}
	      };
	  \end{tikzpicture}
		\vspace{-0.3cm}
		\caption{Invalid UI-CP PLP-tree \label{fig:invalid_UICP}}
	\end{subfigure}%
  \begin{subfigure}[b]{0.23\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
	    level 1/.style={sibling distance=1.7cm, level distance=25pt},
			level 2/.style={sibling distance=0.7cm, level distance=25pt},
			level 3/.style={sibling distance=0.4cm, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$X_3$}
	      child {node [main node,inner sep=0.5pt] (2) {$X_2$}
	    		child {node [main node,inner sep=0.5pt] (3) {$X_4$}
	      		child {node [rectangle,draw] (4) {}}
	      		child {node [rectangle,draw] (5) {}}
					}
	    		child {node [rectangle,draw] (6) {}}
				}
	      child {node [main node,inner sep=0.5pt] (7) {$X_4$}
	      	child {node [rectangle,draw] (8) {}}
	      	child {node [main node,inner sep=0.5pt] (9) {$X_2$}
	      		child {node [rectangle,draw] (10) {}}
	      		child {node [rectangle,draw] (11) {}}
					}
	      };
	  \end{tikzpicture}
		\caption{CI-FP PLP-tree \label{fig:noncoll_PLPT}}
	\end{subfigure}
  \caption{PLP-trees over the dinner domain}
  \label{fig:PLPTree}
\end{figure}


\vspace{-0.1cm}
\subsection{Classification of PLP-Trees}

\vspace{-0.1cm}
In the worst case, the size of a PLP-tree is exponential in the number of 
issues in $\cI$. However, some PLP-trees have a special structure that
allows us to ``collapse" them and obtain more compact 
representations. This yields a natural classification of PLP-trees, which
we describe below.

%Let $R$ be a sequence of issues from $\cI$. By $\hat{R}$ we denote the
%set of issues appearing in $R$. 
Let $R \subseteq \cI$ be the set of issues that appear in a PLP-tree $T$. We say that $T$ is
\tit{collapsible} if there is a permutation $\hat{R}$ of elements in
$R$ such that for every path in $T$ from the root to a leaf, issues 
that label nodes on that path appear in the same order in which they 
appear in $\hat{R}$. 

If a PLP-tree $T$ is collapsible, we can represent $T$ by a single path
of nodes labeled with issues according to the order in which they occur
in $\hat{R}$, 
%(where $\hat{R}$ is the permutation of the set $R$ of issues in $T$),
where a node labeled with an issue $X_i$ is also assigned a
\tit{partial conditional preference table} (PCPT) that specifies 
preferences on $X_i$, conditioned on values of ancestor issues in the path.
These tables make up for the lost structure of $T$ as different ways in 
which ancestor issues evaluate correspond to different locations
in the original tree $T$. 
Moreover, missing entries in PCPT of $X_i$ imply equivalence (or indifference) between values of $X_i$
under conditions that do not appear in the PCPT.
Clearly, the PLP-tree in \figref{UICP_PLPT_full} is collapsible, and can
be represented compactly as a single-path tree with nodes labeled by 
issues in the permutation and PCPTs (cf. \figref{UICP_PLPT_compact}).
Such a collapsed path labeled by issues is sometimes denoted as
a sequence of issues in $\hat{R}$ connected by $\tr$, e.g.,
$X_2\tr X_3\tr X_1$ for the path in \figref{UICP_PLPT_compact}.

Collapsible PLP-trees represented by a single path of nodes 
will be referred to as \emph{unconditional importance} trees or $\UI$ 
trees, for short. The name reflects the fact that the order in which 
we consider issues when seeking the rank of an outcome is always the 
same (not conditioned on the values of ancestor issues of higher importance).

Let $L$ be a collapsible PLP-tree.
If for every path in $L$ the order of issues labeling the path is exactly $\hat{R}$,
and $L$ has the same preference $1>0$ on \emph{every} node,
then every PCPT in the collapsed tree contains the same preference $1>0$,
no matter the evaluation of the ancestor issues.
Thus, every PCPT in the collapsed form can be simplified to a single \emph{fixed} preference $1>0$,
a shorthand for its full-sized counterpart.
We call the resulting
collapsed tree a $\UI$ tree with \emph{fixed preferences}, or a $\UI$-$\FP$ PLP-tree.

A similar simplification is possible if every path in $L$ has the same ordering of issues
which again is exactly $\hat{R}$, and
for every issue $X_i$ all nodes in $L$ labeled with $X_i$ have the same
preference on values of $X_i$ (either $1_i>0_i$ or $0_i>1_i$).
Such collapsed 
trees are called $\UI$-$\UP$ PLP-trees, with $\UP$ standing for \emph{unconditional 
preference}. As an example, the $\UI$-$\UP$ tree in \figref{UIUP_PLPT_compact}
is the collapsed representation of the collapsible tree in \figref{UIUP_PLPT_full}.

In all other cases, we refer to collapsed PLP-trees as $\UI$-$\CP$ PLP-trees, 
with $\CP$ standing for \emph{conditional preference}. If preferences on 
an issue in such a tree depend in an essential way on all preceding 
issues, there is no real saving in the size of representation (instead 
of an exponential PLP-tree we have a small tree but with preference 
tables that are of exponential size). However, if the preference on an 
issue depends only on a few higher importance issues say, never more than 
one or two (or, more generally, never more than some fixed bound $b$), 
the collapsed representation is significantly smaller.

We now show the algorithm to turn a collapsible PLP-tree $T$
into its compact $\UI$-$\CP$ representation $T'$.
We would first eliminate the leaves. 
Since subtrees in $T$ are collapsible as well, we traverse and collapse 
subtrees of inner roots in a post-order manner.
When combining preferences for corresponding nodes,
we will introduce the new condition (upon the root of the subtrees collapsed)
unless the CPTs of these two nodes are the same.
See example.

As an aside, we note that not every path of nodes labeled with issues
and CPTs is a valid $\UI$ tree.
Before define validity of such paths, we examine two properties of CPT's:
completeness and consistency.

Let CPT($X_i$) be the CPT of issue $X_i$ consisting of conditional preference
rules $r_i$'s of form $\cond_i: >_i$.  We define the set of $X_i$'s parent issues
$\Pa(X_i)=\bigcup_{r_i \in CPT(X_i)} \Iss(\cond_i)$,
where $\Iss(\cond_i)$ is the set of issues that appear in $\cond_i$,
and $\Iss(\top)=\emptyset$.
The CPT of issue $X_i$ is \emph{complete} if for every
assignment $v \in \Asst(\Pa(X_i))$ there exists a preference rule $r_i$
such that $v \models \cond_i$.
CPT's that are not complete are \emph{partial}.
Another concept before validity is the
consistency of a CPT. 
A CPT is \emph{inconsistent} if there exist an outcome
and two conditional preference rules in this PCPT such that the outcome satisfies
both conditions but the preference orderings are different.
A CPT is \emph{consistent} if it is not inconsistent.

We obtain the following complexity results for checking completeness and consistency
of a CPT.
The problem of deciding completeness of a CPT is coNP-complete (cf. \thmref{completeness}), and
checking consistency of a CPT can be done in polynomial time.

\begin{thm}
\label{thm:completeness}
	Let PCPT($X$) in a UI-CP PLP-tree $T$ be the PCPT of issue $X$ consisting of conditional preference
	rules $r$'s of form $\cond: >$.
	The problem to decide if PCPT($X$) is complete is coNP-complete.
\end{thm}
\begin{proof}
	(Membership) The complement of the problem is to decide if PCPT($X$) is not complete, that is,
	to decide whether there exists an assignment $w \in \Asst(\Pa(X))$ for every preference rule $r$
	such that $w \not \models \cond$.  This complement problem is clearly in NP.

	(Hardness) We show the hardness of the above complement problem by a reduction from SAT.
	Given an instance of SAT $\Phi=\{C_1,\ldots,C_g\}$ over Boolean variables
	$X_1,\ldots,X_n$, we construct a PCPT as follows.

	1. Introduce a new variable $X_{n+1}$ for which we build the PCPT.

	2. For each clause $C_i \in \Phi$, add into PCPT($X_{n+1}$) a conditional preference
	rule $r_i$ of form $\cond_i: >_i$, where $\cond_i=\neg C_i$ and $>$ is set arbitrarily.

	Clearly, PCPT($X_{n+1}$) is not complete if and only if $\Phi$ is satisfiable.
\end{proof}

Now, let $T$ be a path of $n<p$ nodes labeled with issues and consistent CPT's.
For simplicity, we assume the order of issues is $X_1,\ldots,X_n$.
We say that $T$ is a \emph{valid} $\UI$ tree if
there exists a full PLP-tree that can be collapsed into $T$ using
the algorithm described above.

An example is given in \figref{UICP_PLPT_compact}.
The condition characterizing paths with nodes labeled with
issues and PCPTs that are valid $\UI$ trees is the following.

Let $v$ be a partial interpretation over $\cI$, and $S \subseteq \cI$ a subset of $\cI$.
We denote with $v|_S$ the \tit{projection} of $v$ onto $S$.
In particular, if $v|_\emptyset=\bot$.
We now show the conditions for a path to be a valid UI-CP PLP-trees in the following theorem.

\begin{thm}
\label{thm:valid}
	Let $T$ be a path of nodes labeled with issues and consistent PCPT's.
	We have that $T$ is valid if and only if
	for every node $t \in T$ labeled by $X_i$ and CPT($X_i$),
	for every preference rule $r_i$ in CPT($X_i$),
	for every parent issue $X_j \in \Iss(\cond_i)$ with CPT($X_j$),
	there exists a preference rule $r_j$ in PARTIAL CPT($X_j$)
	such that 
	\begin{center}
		$\cond_i|_{\Iss(\cond_j)} \models \cond_j$.
	\end{center}
	%Note that condition $\cond_i$ can be empty; in this case, we have a single
	%unconditional preference rule $\top:>$ in the PCPT.
\end{thm}
\begin{proof}
Prove by induction. Let $N$, $1\leq N\leq p$, be the number of issues in $T$.
It is clear when $N$ is one.  Now assuming the theorem holds for UI-CP trees of $1\leq n < p$
issues, we show it also holds for an arbitrary UI-CP tree $T$ with $N=n+1$.

($\Leftarrow$) Let $T_n$ be the tree obtained from $T$ by removing the bottom node together
with its issue and PCPT labels.
Let $T_n'$ be the full tree expanded from $T_n$.  Then, we know that the orderings of issues
given by the paths in $T_n'$ are consistent.
\end{proof}
 
When a PLP-tree is not collapsible, the importance of an issue depends
on where it is located in the tree. We will refer to such PLP-trees as
\tit{conditional importance} trees or $\CI$ trees.

Let $T$ be a $\CI$ PLP-tree.
We call $T$ a $\CI$-$\FP$ tree if every non-leaf node in $T$
is labeled by an issue with preference $1>0$.
An example of a $\CI$-$\FP$ PLP-tree is shown in \figref{noncoll_PLPT}, where
preferences on each non-leaf node are $1>0$ and hence omitted.
If, for every issue $X_i$, all nodes in $T$ labeled with $X_i$
have the same preference ($1_i>0_i$ or $0_i>1_i$) on $X_i$, we say $T$ is
a $\CI$-$\UP$ PLP-tree.  
All other non-collapsible PLP-trees are called $\CI$-$\CP$ PLP-trees.
%As mentioned above, the size of $\CI$-$\CP$ trees in the worst case
%could be too large due to the binary tree structure.
%However, if the depth of the tree or the number of paths is bounded 
%by some constant, the size of the model will be substantially reduced.


\section{Passive Learning}

An \tit{example} is a tuple $(\alpha, \beta, v)$, where $\alpha$ and 
$\beta$ are two \emph{distinct} outcomes from combinatorial domain 
$\cX$ over a set $\cI=\{X_1,\ldots,
X_p\}$ of binary issues, and $v \in \{0,1\}$. An example $(\alpha,\beta,1)$
states that $\alpha$ is strictly preferred to $\beta$ ($\alpha \succ \beta$).
Similarly, an example $(\alpha,\beta,0)$ states that $\alpha$ and $\beta$ 
are equivalent ($\alpha\approx\beta$). Let $\cE=\{e_1,\ldots,e_m\}$ be a set 
of examples over $\cI$, with $e_i=(\alpha_i,\beta_i,v_i)$. We set 
$\cE^{\approx}= \{e_i \in \cE: v_i=0\}$, and $\cE^{\succ}= \{e_i \in \cE: 
v_i =1\}$. 
In the following, we denote by $p$ and $m$ the number of issues and the number of
examples, respectively.

For a PLP-tree $T$ in full representation we denote by $|T|$ the size of $T$, that is,
the number of nodes in $T$. If $T$ stands for a $\UI$ tree, we write $|T|$ for
the size of $T$ measured by the total size of preference tables associated
with issues in $T$. The size of a preference table is the total size of
preferences in it, each preference measured as the number of values in the condition
plus $1$ for the preferred value in the domain of the issue.
In particular, the sizes of $\UI$-$\FP$ and $\UI$-$\UP$ trees are
given by the number of nodes on the path. 

A PLP-tree $T$ \tit{satisfies} an example $e$ if $T$ orders the two 
outcomes of $e$ in the same way as they are ordered in $e$. Otherwise,
$T$ \tit{falsifies} $e$. Formally, $T$ \tit{satisfies} $e=(\alpha,\beta,1)$
if $\alpha \succ_T \beta$, and $T$ \tit{satisfies} $e=(\alpha,\beta,0)$ if
$\alpha \approx_T \beta$. We say $T$ is \tit{consistent} with a set $\cE$ 
of examples if $T$ satisfies every example in $\cE$.

In this work, we study the following passive learning problems for PLP-trees 
of all types we introduced.
\begin{definition}
Consistent-learning (\tsc{ConsLearn}): given an example set $\cE$, decide 
whether there exists a PLP-tree $T$ (of a particular type) such that $T$ 
is consistent with $\cE$.
\end{definition}

\begin{definition}
Small-learning (\tsc{SmallLearn}): given an example set $\cE$
and a positive integer $l$ ($l \leq |\cE|$), decide whether there 
exists a PLP-tree $T$ (of a particular type) such that $T$ is consistent 
with $\cE$ and $|T| \leq l$.
\end{definition}

\begin{definition}
Maximal-learning (\tsc{MaxLearn}): given an example set $\cE$ and a 
positive integer $k$ ($k \leq m$), decide whether there exists a PLP-tree 
$T$ (of a particular type) such that $T$ satisfies at least $k$ examples 
in $\cE$.
\end{definition}


\section{Learning UI PLP-trees}
In this section, we study the passive learning problems for collapsible 
PLP-trees in their collapsed representations as $\UI$-$\FP$, $\UI$-$\UP$ and
$\UI$-$\CP$ trees.


\vspace{-0.1cm}
\subsection{The \tsc{ConsLearn} Problem}
\vspace{-0.1cm}
%Since how hard is the problem in the setting of $\UI$-$\CP$ trees remains open
%at the time of this paper, we will study it in the future and
%in this work we focus on the other two $\UI$ cases for the \tsc{ConsLearn} problem.

The \tsc{ConsLearn} problem is in the class P for $\UI$-$\FP$ and
$\UI$-$\UP$ trees. To show it, we present a general 
template of an algorithm that learns a $\UI$ tree. Next, for each of the classes
$\UI$-$\FP$ and $\UI$-$\UP$, we specialize the template to a 
polynomial-time algorithm.

The template algorithm is shown as \algref{learnUI}. The 
input consists of a set $\cE$ of examples and a set $\cI$ of issues 
from which node labels can be selected. Throughout the execution, the 
algorithm maintains a set $S$ of unused issues, initialized to $\cI$, 
and a set of examples that are not yet ordered by the tree constructed 
so far.

If the set of strict examples is empty, the algorithm returns an empty tree. 
Otherwise, the algorithm identifies the set $\AI(\cE,S)$ of issues in 
$S$ that are \emph{available} for selection as the label for the next 
node. If that set is empty, the algorithm terminates with failure. If
not, an issue, say $X_l$, is selected from $\AI(\cE,S)$, and a PCPT for
that issue is constructed. Then the sets of examples not ordered yet 
and of issues not used yet are updated, and the steps repeat.  

\begin{algorithm}[ht]
\KwIn{$\cE$ and $S=\cI$}
\KwOut{A sequence $T$ of issues from $\cI$ and PCPTs that define
a $\UI$ tree consistent with $\cE$, or FAILURE if such a tree does not 
exist}

	$T \leftarrow \mbox{empty sequence}$\;
	\While{$\cE^\succ \neq \emptyset$}{
        Construct $\AI(\cE,S)$\;
        \If{$\AI(\cE,S)=\emptyset$}{
                \Return FAILURE\;
        }
        $X_l \leftarrow \mbox{an element from $\AI(\cE,S)$}$\;
	Construct $\PCPT(X_l)$\; 
	$T \leftarrow T, (X_l, \PCPT(X_l))$\;
        $\cE \leftarrow \cE \backslash \{e \in \cE^\succ: e$ is decided on $X_l\}$\;
        $S \leftarrow S \backslash \{X_l\}$\;
	}
        \Return $T$\;
\caption{Procedure \tit{learnUI} that learns a UI tree 
\label{alg:learnUI}}
\end{algorithm}

To obtain a learning algorithm for a particular class of $\UI$ trees
($\UI$-$\FP$ or $\UI$-$\UP$) we need to specify the notion 
of an available issue (needed for line 3) and describe how to construct 
a partial conditional preference table (needed for line 8). 

To this end, let us define $\NEQ(\cE,S)$ to be the set of all issues in
$S$ (where $S\subseteq \cI$) that incorrectly handle at least one equivalent example 
in $\cE^\approx$. That is, for an issue $X\in S$ we have $X\in \NEQ(\cE,S)$
precisely when for some example $(\alpha,\beta,0)$ in $\cE$, $\alpha(X)\not=
\beta(X)$. Similarly, let us define $\EQ(\cE,S)$ to be the set of 
issues in $S$ that do not order any of the strict examples in $\cE$. That 
is, for an issue $X\in S$ we have $X\in \EQ(\cE,S)$ precisely when 
for every example $(\alpha,\beta,1)$ in $\cE$, $\alpha(X)= \beta(X)$.  

\noindent {\bf Fixed Preferences.}
For the problem of learning $\UI$-$\FP$ trees, we define $\AI(\cE,S)$ to
contain every issue $X\notin \NEQ(\cE,S)$ such that\\ 
(1) \ for every $(\alpha,\beta,1) \in \cE^\succ$, $\alpha(X) \geq \beta(X)$.

\begin{prop}
\label{prop:1}
{\it If there is a $\UI$-$\FP$ tree consistent with all examples in $\cE$ and 
using only issues from $S$ as labels, then an issue $X\in S$ is a top node
of some such tree if and only if $X\in \AI(\cE,S)$.}
\end{prop}
\begin{proof}
Let $T$ be a $\UI$ tree consistent with $\cE$ and having only issues from 
$S$ as labels. Let $X$ be the issue labeling the top node of $T$. Clearly,
$X\notin \NEQ(\cE,S)$, as otherwise, $T$ would strictly order two outcomes
$\alpha$ and $\beta$ such that $(\alpha,\beta,0)\in \cE^\approx$. To prove
condition (1), let us consider any example $(\alpha,\beta,1)\in \cE^\succ$. 
Since $T$ is consistent with $(\alpha,\beta,1)$, $\alpha(X)\geq\beta(X)$. 
Consequently, $X\in \AI(\cE,S)$.

Conversely, let $X\in \AI(\cE,S)$ and let $T$ be a $\UI$-$\FP$ tree 
consistent with all examples in $\cE$ and using only issues from $S$ as 
labels (such a tree exists by assumption). If $X$ labels the top node in 
$T$, we are done. Otherwise, let $T'$ be a tree obtained from $T$ by 
adding at the top of $T$ another node, labeling it with $X$ and removing
from $T$ the node labeled by $X$, if such a node exists. 
Let $T$ be $X_{j_1} \tr \ldots \tr X_{j_k}$ ($k \leq p$).

\noindent (1). If $X \not \in \{T\}$, we have $T'=X \tr X_{j_1} \tr \ldots \tr X_{j_k}$.
For any $e=(\alpha,\beta,0) \in \cE^\approx$, we know $\alpha(X)=\beta(X)$ since
$X\in \AI(\cE,S)$, and $\alpha(X_{j_i})=\beta(X_{j_i})$ for all
$1 \leq i \leq k$ because $\alpha \approx_T \beta$.
Thus, we have $\alpha \approx_{T'} \beta$.
Let $e'=(\alpha',\beta',1) \in \cE^\succ$ be any strict example 
decided on issue $X_{j_l}$ in $T$.
That is, we know $\alpha'(X_{j_l})=1$ and $\beta'(X_{j_l})=0$, and
$\alpha(X_{j_i})=\beta(X_{j_i})$ for all $1 \leq i < l$.
By the definition of $\AI(\cE,S)$, we have $\alpha'(X) \geq \beta'(X)$.
If $\alpha'(X) = \beta'(X)$, example $e'$ is still decided on issue $X_{j_l}$ in $T'$.
Otherwise, we have $\alpha'(X)=1$ and $\beta'(X)=0$, and $e'$ is decided on
issue $X$ in $T'$. Hence, we have that $\alpha \succ_{T'} \beta$.

\noindent (2). If $X \in \{T\}$, let $j_r$, $1\leq r\leq k$, be the index such that $X=X_{j_r}$.
Now we have $T'=X \tr X_{j_1} \tr \ldots \tr X_{j_{r-1}} \tr X_{j_{r+1}} \tr \ldots \tr X_{j_k}$.
Tree $T'$ is consistent with $\cE^\approx$, done in case (1).
Let $\cE^\succ_{j_l}$ be the set of examples in $\cE^\succ$ that are
decided on issue $X_{j_l}$ in $T$.
We have that examples in every $\cE^\succ_{j_l}$, $1\leq l \leq r-1$, are decided in $T'$,
again done in case (1).
For $l=r$, examples in $\cE^\succ_{j_l}$ are decided on $X$ in $T'$.
For $r+1\leq l\leq k$, examples in every $\cE^\succ_{j_l}$ are decided on $X_{j_l}$.

Therefore, tree $T'$ is consistent with $\cE$.
\end{proof}

We now specialize \algref{learnUI} by using in line 3 the 
definition of $\AI(\cE,S)$ given above and by setting each $\PCPT(X_l)$ 
to the fixed unconditional preference $1_l>0_l$. Proposition \ref{prop:1}
directly implies the correctness of this version of \algref{learnUI}.

\begin{thm}
Let $\cE$ be a set of examples over a set $\cI$ of binary issues.
\algref{learnUI} adjusted as described above terminates and outputs
a sequence $T$ representing a $\UI$-$\FP$ tree consistent with $\cE$ if
and only if such a tree exists.
\end{thm}

We note that issues in $\NEQ(\cE,S)$ are never used when constructing
$\AI(\cE,S)$. Thus, in the case of $\UI$-$\FP$ trees, $S$ could be
initialized to $\cI\setminus\NEQ(\cE,\cI)$. In addition, if an issue
selected for the label of the top node belongs to $\EQ(\cE^\succ, S)$,
it does not in fact decide any of the strict examples in $\cE$ and can be
dropped. The resulting tree is also consistent with all the examples. 
Thus, the definition of $\AI(\cE,S)$ can be refined by requiring one more
condition: $X \not \in \EQ(\cE^\succ, S)$. That change does not affect 
the correctness of the algorithm but eliminates a possibility of generating
trees with ``redundant'' levels.


\noindent {\bf Unconditional Preferences.}
The case of learning $\UI$-$\UP$ trees is very similar to the 
previous one. Specifically, we define $\AI(\cE,S)$ to contain an issue 
$X\in S$ precisely when $X\notin\NEQ(\cE,S)$ and

\noindent 
(2) \ for every $(\alpha,\beta,1) \in \cE^\succ$, $\alpha(X) \geq 
\beta(X)$, or for every $(\alpha,\beta,1) \in \cE^\succ$, $\alpha(X) \leq \beta(X)$.

We obtain an algorithm learning $\UI$-$\UP$ trees by using in line 3
the present definition of $\AI(\cE,S)$. In line 8, we take for 
$\PCPT(X_l)$ either $1_l>0_l$ or $0_l>1_l$ (depending on which of the two
cases in (2) holds for $X_l$).

The correctness of this algorithm follows from a property similar to
that in \propref{1}.

As in the previous case, here too $S$ could be initialized to $\cI \setminus
\NEQ$, and the condition $X\not\in \EQ(\cE^\succ,S)$ could be added to the 
definition of $\AI(\cE,S)$. 

\noindent {\bf Conditional Preferences.} 
The problem is in NP because one can show that if a $\UI$-$\CP$
tree consistent with $\cE$ exists (\emph{a priori},
it does not have to have size polynomial in the size of $\cE$), then another
such tree of size polynomial in the size of $\cE$ exists, as well.
%We conjecture that the general problem of
%learning $\UI$-$\CP$ trees is, in fact, NP-complete. 
%As we have only partial results 
%for this case, the study of the $\UI$-$\CP$ tree learning will be the subject
%of future work. 
We show that the problem is in fact NP-complete.

We first show that the Weak Consistency problem for a special class of 
acyclic CP-nets is NP-complete.
A CP-net $N=(G,\CPT)$ contains a directed graph $G$ over $\cI$ determining the
dependencies between issues, and a set $\CPT$ of conditional preference tables $\CPT(X_i)$ for
every $X_i \in \cI$. For instance, if $X_i$ has $r$ incoming edges in $G$, then
$\CPT(X_i)$ is a table of $2^r$ conditional preference orderings on $D_i$ 
for all assignments to these $r$ parent nodes.
We say a CP-net $N=(G,\CPT)$ is \emph{total acyclic} if $G$ is acyclic and
$G=G^+$, where $G^+$ is the transitive closure of $G$.

Let $N$ be a total acyclic CP-net,
$\succ_N$ be the preference order induced by $N$,
and $e=(\alpha,\beta,1)$ be a strict example.
We say that $N$ is consistent with $e$ if there is a completion $\succ$ of 
$\succ_N$ such that $\alpha \succ \beta$.
\begin{definition}
Let $N$ be a total acyclic CP-net over $\cI$, and $\cE^\succ$ be a set of strict examples.
We say that $N$ is weakly consistent with $\cE^\succ$ if $N$ is consistent 
with every example in $\cE^\succ$.
\end{definition}

\begin{thm}
Let $\cE^\succ$ be a set of strict examples.
The weak consistency problem, to decide whether there exists
a total acyclic CP-net $N$ that is weakly consistent with $\cE^\succ$, is NP-complete.
\end{thm}
\begin{proof}
	For the membership, we can guess a directed dependency graph $G$ over $\cI$ and 
	verify that $G$ is total acyclic in polynomial time. We can show that if there is a total acyclic CP-net
	$N=(G,\CPT)$ weakly consistent with $\cE^\succ$ (\emph{a priori}, the tables $\CPT$
	may have size exponential in the size of $\cE^\succ$), 
	then another such CP-net of polynomial size exists too.
	The hardness comes from the fact that a special case, the \emph{weak separability 
	problem} \cite{LangM09}, is NP-complete.
\end{proof}

\begin{lem}
\label{lem:cp}
Let $N=(G,\CPT)$ be a total acyclic CP-net, and $e=(\alpha,\beta,1)$ be a strict example.
We have that $N$ is consistent with $e$ if and only if there exists an issue $X_i \in \cI$
such that $\alpha(U_i)=\beta(U_i)$ and $\alpha(X_i) >_i \beta(X_i)$ given $\alpha(U_i)$ 
(or $\beta(U_i)$),
where $U_i$ is the set of parent issues of $X_i$ in $G$, and $>_i$ is the total order over
$D_i$.
\end{lem}
\begin{proof}
	($\Leftarrow$) It is proved in the setting of \emph{ordering queries} \cite{bbdh03}.

	($\Rightarrow$) Assume for every issue $X_j \in \cI$ we have $\alpha(U_j)=\beta(U_j)$ and
	$\alpha(X_j) \not >_j \beta(X_j)$ given $\alpha(U_j)$ (or $\beta(U_j)$), we need to
	show that $\beta \succ_N \alpha$, that is, there is a sequence of improving flips
	from $\alpha$ to $\beta$ in the preference graph $\succ_N$.

	Case 1: Assume for every issue $X_j \in \cI$ we have $\alpha(U_j)=\beta(U_j)$ and 
	$\alpha(X_j) = \beta(X_j)$ given $\alpha(U_j)$ (or $\beta(U_j)$).
	Then, it must be that $\alpha=\beta$, which is impossible.

	Case 2: Assume for some issue $X_k$ we have $\alpha(U_k)=\beta(U_k)$ and
	$\beta(X_k) >_k \alpha(X_k)$ given $\alpha(U_k)$ (or $\beta(U_k)$).
	Then, it must be that $X_k \in G$ has an out degree of 0.
	It is easy to see there is a sequence of improving flips from $\alpha$ to $\beta$ in $\succ_N$.
\end{proof}

\begin{thm}
	The \tsc{ConsLearn} problem is NP-complete for class $\UI$-$\CP$.
\end{thm}
\begin{proof}
	Membership is clear. We now prove that the problem is NP-hard by showing a polynomial
	time reduction from the \emph{weak consistency problem}.
	Let $\cE^\succ$ be a set of strict examples.  We show that there exists a
	total acyclic CP-net $N$ weakly consistent with $\cE^\succ$ if and only if
	there exists a UI-CP PLP-tree $T$ consistent with $\cE^\succ$.

	($\Rightarrow$) Assume $N$ is weakly consistent with $\cE^\succ$, we
	construct $T$, where the order of issues is a linear completion of $G$
	and the $\PCPT$s are copies of $\CPT$s in $N$ for corresponding issues.
	Since $e=(\alpha,\beta,1) \in \cE^\succ$ is consistent with $N$, by \lemref{cp}, there is a preference
	rule $u:>_i \in \CPT(X_i)$ such that $\alpha(U)=\beta(U)=u$ and $\alpha(X_i) >_i \beta(X_i)$.
	It is clear that $e$ is decided on $X_i$ in tree $T$.

	($\Leftarrow$) Assume $T$ is consistent with $\cE^\succ$, we build $N$, where
	$G$ is constructed in a way that $G$ is consistent with the dependencies between issues
	in $T$, and $\CPT$s are copies of $\PCPT$s in $T$ for same issues.
	If $e=(\alpha,\beta,1) \in \cE^\succ$ is decided on $X_i$ in $T$, on preference rule
	$u:>_i \in \CPT(X_i)$ in $N$ we have $\alpha(U)=\beta(U)=u$ and $\alpha(X_i) >_i \beta(X_i)$.
	Thus, by \lemref{cp}, $N$ is weakly consistent with all $e$ in $\cE^\succ$ and hence $\cE^\succ$.
\end{proof}

%However, we consider class $\UI$-$\CP$-$\Prefix$, a subclass of $\UI$-$\CP$,
%and show that the \tsc{ConsLearn} problem can be solved by adjusting
%\algref{learnUI} to the case of learning $\UI$-$\CP$-$\Prefix$ trees.
%If, for every path in a PLP-tree $T$ from the root to a leaf, 
%the sequence of issues 
%that label the nodes on that path is a \tit{prefix} of $\hat{R}$, i.e.,
%a segment of $\hat{R}$ starting at the top issue in $\hat{R}$,
%then tree $T$ can be collapsed into a $\UI$ tree with PCPTs labeling
%issues. The $\UI$-$\CP$ tree in \figref{UICP_PLPT_compact} is also a
%$\UI$-$\CPsa tree.
%
%We define $\AI(\cE,S)$ for class $\UI$-$\CP$-$\Prefix$.
%Let $P=\cI\backslash S$ be the set of issues that are already learned.
%Given $\cE$ and the set $P$, we partition $\cE$ into $\{\cE_1,\ldots,\cE_r\}$ 
%according to $P$ such that the following holds:
%
%\noindent (1). $\forall e_i \in \cE_i, \alpha_{e_i}(P) = \beta_{e_i}(P)$; 
%
%\noindent (2). $\forall e_i,e_j \in \cE_i, \alpha_{e_i}(P) = \alpha_{e_j}(P)$;
%
%\noindent (3). $\forall e_i \in \cE_i, \forall e_j \in \cE_j, \alpha_{e_i}(P)\not = \alpha_{e_j}(P)$.
%
%We have $X \in \AI(\cE,S)$ if 
%for every $\cE_i$, $1 \leq i \leq r$, such that $\cE_i^\succ \not =\emptyset$, the following hold:
%
%\noindent (1) for every $(\alpha,\beta,1) \in \cE_i$, $\alpha(X) \geq 
%\beta(X)$, or for every $(\alpha,\beta,1) \in \cE_i$, $\alpha(X) \leq \beta(X)$.
%
%\noindent (2). $X \not \in \NEQ(\cE_i, S)$.
%
%\begin{prop}
%\label{prop:3}
%	{\it
%	Let $\cE_i$ be a partition of $\cE$ defined above with $\cE^\succ_i \not = \emptyset$.
%	If for each such $\cE_i$ there is a UI-CP-Prefix tree consistent with $\cE_i$ and
%	using only issues from $S$ as labels, and these UI-CP-Prefix trees have the same issue labeling their roots,
%	then an issue $X \in S$ is a top node of some such tree for every $\cE_i$ if and only if $X \in \AI(\cE,S)$.
%	}
%\end{prop}
%\begin{proof}
%	It is clear that if for each $\cE_i$ ($\cE^\succ_i \not = \emptyset$) there is a 
%	$\UI$-$\CP$-$\Prefix$ tree consistent with $\cE_i$ and only using issues from $S$ as labels, 
%	and these UI-CP trees have the same issue labeling their roots,
%	then the fact that
%	$X \in S$ labels the root of some such tree implies $X \in \AI(\cE,S)$.
%
%	Now we show the other direction.
%	Let $T$ be the $\UI$-$\CP$-$\Prefix$ tree over a subset of $S$ consistent with 
%	an arbitrary $\cE_i$, $1\leq i \leq r$, such that $\cE_i^\succ\not =\emptyset$,
%	and $X$ be an isssue such that $X \in \AI(\cE,S)$.
%	Like class $\UI$-$\FP$, if $X$ is the root issue in $T$, we are done.
%	Otherwise, let $T'$ be a tree obtained from $T$ by (1) adding at the top
%	of $T$ another node, labeling it with $X$ and, w.l.o.g., $1>0$ as its PCPT,
%	(2) updating the PCPTs of all nodes below $X$ that are ranked higher in $T$ 
%	than the one labeled by $X$, if any; otherwise, updating all nodes below $X$,
%	and (3) removing from $T$ the node labeled by $X$ and its PCPT, if any.
%\end{proof}
%
%\propref{3} clearly infers the correctness of \algref{learnUI} with $\AI(\cE,S)$
%defined as above for class $\UI$-$\CP$-$\Prefix$ and each $\PCPT(X_l)$ set consistent with
%the condition aforementioned.
%
%\begin{thm}
%Let $\cE$ be a set of examples over a set $\cI$ of binary issues.
%\algref{learnUI} adjusted as described above terminates and outputs
%a $\UI$-$\CP$-$\Prefix$ tree $T$ consistent wit $\cI$ if
%and only if such a tree exists.
%\end{thm}


\vspace{-0.1cm}
\subsection{The \tsc{SmallLearn} Problem}

\vspace{-0.1cm}
\algref{learnUI} produces a $\UI$ PLP-tree consistent with $\cE$, if one exists.
In many cases, it is desirable to compute a small, sometimes even the smallest, 
representation consistent with $\cE$.
We show that these problems for $\UI$ trees are NP-hard.

\begin{thm}
\label{thm:UIFP_smallest_decision}
	The \tsc{SmallLearn} problem is NP-complete for each class of $\{\UI\} \times \{\FP,\UP,\CP\}$.
\end{thm}
\begin{proof}
	We present the proof only in the case of $\UI$-$\FP$. The argument in other cases 
	($\UI$-$\UP$ and $\UI$-$\CP$) is similar.

	(Membership) One can guess a $\UI$-$\FP$ PLP-tree $T$ in linear time, and verify in polynomial time that 
	$T$ has at most $l$ issues and satisfies every example in $\cE$.

	(Hardness) We present a polynomial-time reduction from the
	\tit{hitting set problem} (HSP), which is NP-complete 
    \cite{Garey:1979}. To recall, in HSP we are given a finite
		set $U=\{a_1,\ldots,a_n\}$, a collection $C=\{S_1,\ldots,
    S_d\}$ of subsets of $U$ with $\bigcup_{S_i \in C} S_i = U$,
    and a positive integer $k \leq n$, and the problem is to
    decide whether $U$ has a hitting set $U'$ such that $|U'|\leq k$ 
    ($U'\subseteq U$ is a \emph{hitting} set for $C$ if 
		$U' \cap S_i \not = \emptyset$ for all $S_i \in C$).
	Given an instance of HSP, we construct an instance of our problem as follows.

	\noindent 1. $\cI=\{X_i: a_i \in U\}$ (thus, $p=n$).

	\noindent 2. $\cE=\{(\tbf{s}_i, \tbf{0},1): S_i \in C\}$, where
	$\tbf{s}_i$ is a $p$-bit vector such that $\tbf{s}_i[j]=1 \Leftrightarrow a_j \in S_i$
	and $\tbf{s}_i[j]=0 \Leftrightarrow a_j \not \in S_i$ ($1 \leq j \leq p$), 
	and $\tbf{0}$ is a $p$-bit vector of all $0$'s (thus, $m=d$).

	\noindent 3. We set $l=k$.

	We need to show that $U$ has a hitting set of size at most $k$ if and only if
	there exists a $\UI$-$\FP$ PLP-tree of size at most $l$ consistent with $\cE$.

\smallskip
\noindent
	($\Rightarrow$) Assume $U$ has a hitting set $U'$ of size $k$.
	Let $U'$ be $\{a_{j_1},\ldots,a_{j_k}\}$.
	Define a $\UI$-$\FP$ PLP-tree $L=X_{j_1} \tr \ldots \tr X_{j_k}$.
	We show that $L$ is consistent with $\cE$.
	Let $e=(\alpha_e,\beta_e,1)$ be an arbitrary example in $\cE$, where
	$\alpha_e=\tbf{s}_i$ and $\beta_e=\tbf{0}$.
	Since $U'$ is a hitting set, %for every set $S_i\in C$ 
        there exists $r$, $1 \leq r \leq k$, 
	%$a_{j_r}$ ($1 \leq r \leq k$) 
        such that $a_{j_r} \in S_i$.
	Thus, %for $\alpha_e$ 
        there exists $r$, $1 \leq r \leq k$, %$X_{j_r}$ ($1 \leq r \leq k$)
	such that $\alpha_e(X_{j_r})=1$.
	Let $r$ be the smallest with this property. 
	%Among these issues $X_{j_r}$ let $X_j$ be the top-ranked one.
	It is clear that $e$ is decided at $X_{j_r}$; thus, we have 
	$\alpha_e \succ_L \beta_e$.

\smallskip
\noindent
	($\Leftarrow$) Assume there is a $\UI$-$\FP$ PLP-tree $L$ of $l$ issues in
	$\cI$ such that $L$ is consistent with $\cE$. Moreover, we assume
	$L=X_{j_1} \tr \ldots \tr X_{j_l}$.
	Let $U'=\{a_{j_1}, \ldots,a_{j_l}\}$.
	We show by means of contradiction.  Assume that $U'$ is not a hitting set.
	That is, there exists a set $S_i \in C$ such that
	$U' \cap S_i = \emptyset$.
	Then, there exists an example $e=(\alpha_e,\beta_e,1)$, where $\alpha_e=\tbf{s}_i$ 
	and $\beta_e=\tbf{0}$, such that $\alpha_e \approx_L \beta_e$ because
	none of the issues $\{X_i:\alpha_e(X_i)=1\}$ show up in $L$. This is
	a contradiction! Thus, $U'$ is a hitting set.
\end{proof}

\begin{cor}
\label{cor:UIFP_smallest}
	Given a set $\cE$ of examples $\{e_1,\ldots,e_m\}$ over $\cI=\{X_1,\ldots,X_p\}$,
	finding the smallest PLP-tree in each class of $\{\UI\} \times \{\FP,\UP,\CP\}$ 
	consistent with $\cE$ is NP-hard.
\end{cor}

Consequently, it is important to study fast heuristics that aim at
approximating trees of optimal size. 
Here, we propose a greedy heuristic for \algref{learnUI}.
In every iteration the heuristic selects the issue $X_l \in \AI(\cE,S)$ that
decides the most examples in $\cE^\succ$.
However, for some dataset the resulting greedy algorithm does not perform 
well: the ratio of the size of the tree computed by our algorithm 
to the size of the optimal sequence may be as large as $\Omega(p)$.
To see this, we consider the following input.

\begin{framed}
	\vspace{-0.2cm}
	\noindent $(1_10_20_30_4,0_10_20_30_4,1)$\\
	$(1_11_20_30_4,0_10_20_30_4,1)$\\
	$(1_10_21_30_4,0_10_20_30_4,1)$\\
	$(0_10_20_31_4,1_10_20_30_4,1)$
	\vspace{-0.2cm}
\end{framed}

For each class of $\{\UI\} \times \{\FP,\UP\}$, \algref{learnUI} in the worst 
case computes $X_2 \tr X_3 \tr X_4 \tr X_1$,
whereas the optimal tree is $X_4 \tr X_1$ (with
the PCPTs omitted as they contain only one preference and so, they do not
change the asymptotic size of the tree). This example generalizes to the
arbitrary number $p$ of issues. Thus, the greedy algorithm to learn small
$\UI$ trees is no better than any other algorithm in 
the worst case.

Approximating HSP has been extensively studied over the last decades.
It has been shown \cite{lund1994hardness} that, unless $\NP \subset \DTIME(n^{\poly \log n})$,
HSP cannot be approximated 
in polynomial time within factor of $c \log n$, where $0<c<\frac{1}{4}$ and
$n$ is the number of elements in the input. The reduction we used above
shows that this result
carries over to our problem.
\begin{thm}
\label{thm:UI_smallest_approx}
	Unless $\NP \subset \DTIME(n^{\poly \log n})$,
	the problem of finding the smallest PLP-tree in each class of $\{\UI\} \times \{\FP,\UP,\CP\}$ 
	consistent with $\cE$ cannot be approximated 
	in polynomial time within factor of $c \log p$, where $0<c<\frac{1}{4}$.
\end{thm}
It is an open problem whether this result can be strengthened to a factor 
linear in $p$ (cf. the example for the worst-case behavior of our simple 
greedy heuristic).


\vspace{-0.1cm}
\subsection{The \tsc{MaxLearn} Problem}

\vspace{-0.1cm}
When there is no $\UI$ PLP-tree consistent with the set of all examples,
it may be useful to learn a $\UI$ PLP-tree satisfying as many examples 
as possible. We show this problem is in fact NP-hard for all three 
classes of $\UI$ trees.

\begin{thm}
\label{thm:UIFP_least_decision}
The \tsc{MaxLearn} problem is NP-complete for each class of $\{\UI\} \times \{\FP,\UP,\CP\}$.
\end{thm}
\begin{proof}[Sketch]
The problem is in NP. This is evident for the case
of $\UI$-$\FP$ and $\UI$-$\UP$ trees. If $\cE$ is a given set of examples,
and $k$ a required lower bound on the number of examples that are to be 
correctly ordered, then witness trees in these classes (trees that correctly 
order at least $k$ examples in $\cE$) have size polynomial in the size of 
$\cE$. Thus, verification can be performed in polynomial time. 
For the case of $\UI$-$\CP$ trees, one can show that if there is a $\UI$-$\CP$
tree correctly ordering at least $k$ examples in $\cE$, then there exists
such tree of size polynomial in $|\cE|$.

The hardness part follows from the proof in the setting of learning 
lexicographic strategies \cite{schmitt2006complexity}, adapted to the case 
of $\UI$ PLP-trees.
We now show hardness for the class $\UI$-$\CP$ on top of the construction
of examples shown by Schmitt and Martignon \shortcite{schmitt2006complexity}.

We first recall the \tit{vertex cover problem} (VCP): 
given an undirected graph $G=(V,E)$ with
$n$ vertices and $g$ edges, and a positive integer $k\leq n$, the problem is to
decide whether there is a vertex cover of cardinality at most $k$, i.e.,
a subset $V' \subseteq V$ with $|V'|\leq k$ such that for every edge 
$(v_i,v_j) \in E$ at least one of $v_i$ and $v_j$ belongs to $V'$.
Given an instance of VCP, we construct an instance of our problem as follows.

\noindent 1. $\cI=\{X_i:v_i \in V\} \cup \{X_{n+1}\}$.

\noindent 2. $\cE=\{(\la \tbf{1}_i,1 \ra, \la \tbf{1},0 \ra, 1): i=1,\ldots,n\}
\cup \{(\la \tbf{1},0 \ra, \la \tbf{1}_{i,j},1 \ra, 1): (v_i,v_j) \in E\}$,
where \tbf{1} and $\tbf{1}_i$ ($\tbf{1}_{i,j}$) stand for
the $n$-bit vector of all $1$'s, the $n$-bit vector of all $1$'s except 
for position $i$ (respectively, positions $i$ and $j$) where it has a $0$.

\noindent 3. We set $l=k$.

We show that graph $G$ has a vertex cover of size at most $k$ if and only if
the set $\cE$ over $\cI$ has a $\UI$-$\CP$ PLP-tree that falsifies at
most $l$ examples in $\cE$.

\noindent ($\Rightarrow$) Assume $G$ has a vertex cover $V'$ of size $k$ such that
$V'=\{v_{i_1},\ldots,v_{i_k}\}$. We build a $\UI$-$\FP$ tree (a special $\UI$-$\CP$ tree)
$T=X_{i_1} \tr \ldots \tr X_{i_k} \tr X_{n+1}$ with preference $1>0$ on every issue.
Since $V'$ is a vertex cover, any edge example $(\la \tbf{1},0 \ra, \la \tbf{1}_{i,j},1 \ra, 1)$
and any vertex example $(\la \tbf{1}_i,1 \ra, \la \tbf{1},0 \ra, 1)$ with
$v_i \not \in V'$ are satisfied by $T$, and any vertex example with $v_i \in V'$
is falsified by $T$.  Thus, the set $\cE$ has a $\UI$-$\CP$ tree falsifying no more than
$l=k$ examples.

\noindent ($\Leftarrow$) Assume $T$ is a $\UI$-$\CP$ tree that falsifies at most $l$
examples in $\cE$. Build the set $V'$ as follows.

(1). Let $v_i \in V'$ for every falsified vertex example $(\la \tbf{1}_i,1 \ra, \la \tbf{1},0 \ra, 1)$.

(2). Let one of $v_i, v_j \in V'$ for every falsified edge example 
$(\la \tbf{1},0 \ra, \la \tbf{1}_{i,j},1 \ra, 1)$.

We assume that $V'$ of size at most $l$ is not a vertex cover. 
Then, there exists an edge $(v_i,v_j) \in E$ such that its edge example and
two vertex examples are satisfied by $T$.
Let $T'$ be the full representation of $T$.
Since the edge example $e=(\la \tbf{1},0 \ra, \la \tbf{1}_{i,j},1 \ra, 1)$ is
satisfied by $T'$, there exists a path in $T'$ from root to a node labeled by
the issue that decides $e$. This issue, say $X$, must be either $X_i$, $X_j$ or $X_{n+1}$.

\noindent ($1^\circ$). If $e$ is decided on $X_i$, the preference on $X_i$ must be
$1_i>0_i$. Then, the vertex example $(\la \tbf{1}_i,1 \ra, \la \tbf{1},0 \ra, 1)$ is
falsified by $T'$, contradiction.

\noindent ($2^\circ$). If $e$ is decided on $X_j$, the preference on $X_j$ must be
$1_j>0_j$. Then, the vertex example $(\la \tbf{1}_j,1 \ra, \la \tbf{1},0 \ra, 1)$ is
falsified by $T'$, contradiction.

\noindent ($3^\circ$). If $e$ is decided on $X_{n+1}$, the preference on
$X_{n+1}$ must be $0_{n+1}>1_{n+1}$. Then, both vertex examples are falsified
by $T'$, contradiction.

Thus, the set $V'$ is a vertex cover. Therefore, the theorem holds.
\end{proof}

\begin{cor}
\label{cor:UIFP_least}
Given a set $\cE$ of examples $\{e_1,\ldots,e_m\}$ over 
$\cI=\{X_1,\ldots,X_p\}$, finding a PLP-tree in each class of $\{\UI\} \times \{\FP,\UP,\CP\}$ 
satisfying the maximum number of examples in $\cE$ is NP-hard.
\end{cor}

\section{Learning CI PLP-trees}
Finally, we present results on the passive learning problems for 
PLP-trees in classes $\{\CI\} \times \{\FP,\UP,\CP\}$. We recall that these
trees assume full (non-collapsed) representation.


\vspace{-0.1cm}
\subsection{The \tsc{ConsLearn} Problem}

\vspace{-0.1cm}
We first show that the \tsc{ConsLearn} problem for class $\CI$-$\UP$ is 
NP-complete. We then propose polynomial-time algorithms to solve the 
\tsc{ConsLearn} problem
%so as to show that the problem is in class P, 
for the classes $\CI$-$\FP$ and $\CI$-$\CP$.

\begin{thm}
\label{thm:passlearn_CIUP}
	The \tsc{ConsLearn} problem is NP-complete for class $\CI$-$\UP$.
\end{thm}
\begin{proof}[Sketch]
The problem is in NP because the size of a witness, 
a $\CI$-$\UP$ PLP-tree consistent with $\cE$, is bounded by $|\cE|$ (one
can show that if a $\CI$-$\UP$ tree consistent with $\cE$ exists, then it can
be modified to a tree of size no larger than $O(|\cE|)$).
	Hardness follows from the proof by Booth et al. \shortcite{booth:learningLP} showing 
	\tsc{ConsLearn} is NP-hard in the setting of LP-trees.
\end{proof}

For the two other classes of trees, the problem is in P. This is demonstrated 
by polynomial-time \algref{recur_learnCI} adjusted for both classes.

\begin{algorithm}[ht]
\KwIn{$\cE$, $S=\cI$, and $t$: an unlabeled node}
\KwOut{A CI PLP-tree over $S$ consistent with $\cE$, or FAILURE}
	\If{$\cE^\succ = \emptyset$}{
		Label $t$ as a leaf and \Return\;
	}
	Construct $\AI(\cE,S)$\;
	\If{$\AI(\cE,S)=\emptyset$}{
		\Return FAILURE and terminate\;
	}
	Label $t$ with tuple $(X_l,x_l)$ where $X_l$ is from $\AI(\cE,S)$, 
		and $x_l$ is the preferred value on $X_l$\;
	$\cE \leftarrow \cE \backslash \{e \in \cE^\succ: e$ is decided on $X_l\}$\;
	$S \leftarrow S \backslash \{X_l\}$\;
	Create two edges $u_l,u_r$ and two unlabeled nodes $t_l,t_r$ such that $u_l=\langle t,t_l\rangle$
		and $u_r=\langle t,t_r\rangle$\;
	$\cE_l \leftarrow \{e\in \cE: \alpha_e(X_j)=\beta_e(X_j)=x_l\}$\;
	$\cE_r \leftarrow \{e\in \cE: \alpha_e(X_j)=\beta_e(X_j)=\overline{x_l}\}$\;
	$\mathit{learnCI}(\cE_l,S,t_l)$\;
	$\mathit{learnCI}(\cE_r,S,t_r)$\;

\caption{The recursive procedure \tit{learnCI} that learns a CI PLP-tree \label{alg:recur_learnCI}}
\end{algorithm}

\noindent {\bf Fixed Preference.}
For class $\CI$-$\FP$, we define $\AI(\cE,S)$ to contain issue 
$X\notin \NEQ(\cE,S)$ if\\
(3) \ for every $(\alpha,\beta,1) \in \cE^\succ$,  $\alpha(X) \geq \beta(X)$.

\begin{prop}
\label{prop:2}
{\it If there is a $\CI$-$\FP$ tree consistent with all examples in $\cE$ and 
using only issues from $S$ as labels, then an issue $X\in S$ is a top node
of some such tree if and only if $X\in \AI(\cE,S)$.}
\end{prop}
\begin{proof}
	It is clear that if there exists a $\CI$-$\FP$ PLP-tree consistent with $\cE$
	and only using issues from $S$ as labels, then the fact that
	$X \in S$ labels the root of some such tree implies $X \in \AI(\cE,S)$.

	Now we show the other direction.
	Let $T$ be the $\CI$-$\FP$ tree over a subset of $S$ consistent 
	with $\cE$,
	$X$ be an issue such that $X \in \AI(\cE,S)$.
	If $X$ is the root issue in $T$, we are done.
	Otherwise, we construct a $\CI$-$\FP$ tree $T'$ by creating a root, 
	labeling it with $X$, and make one copy of $T$ the left subtree of $T'$
        ($T_l'$) and another, the right subtree of $T'$ ($T_r'$).
	For a node $t$ and a subtree $B$ in $T$, we write $t_l'$ and $B_l'$, 
	respectively, for the corresponding node and subtree in $T_l'$.
	We define $t_r'$ and $B_r'$ similarly. If $X$ does not appear in 
	$T$, we are done constructing $T'$; otherwise, we update $T'$ as 
	follows.

	\noindent 1). For every node $t\in T$ labeled by $X$ such that $t$ has two leaf children,
	we replace the subtrees rooted at $t_l'$ and $t_r'$ in $T_l'$ and $T_r'$ with leaves.

	\noindent 2). For every node $t \in T$ labeled by $X$ such that $t$ has one leaf child
	and a non-leaf subtree $B$, we replace the subtree rooted at $t_l'$ in $T_l'$ with $B_l'$,
	and the subtree rooted at $t_r'$ in $T_r'$ with a leaf,
	if $t \in T$ has a right leaf child; otherwise, we replace the subtree rooted at
	$t_l'$ in $T_l'$ with a leaf, and the subtree rooted at $t_r'$ in $T_r'$ with $B_r'$.

	\noindent 3). Every other node $t \in T$ labeled by $X$ has two non-leaf subtrees:
	left non-leaf subtree $\BL$ and right $\BR$.
	For every such node $t \in T$, we replace the subtree rooted at $t_l'$ in $T_l'$ with
	$\BL_l'$, and the subtree rooted at $t_r'$ in $T_r'$ with $\BR_r'$.

	As an example, this construction of $T'$ from $T$ is demonstrated in \figref{promote}.
	One can show that this construction results in a $\CI$-$\CP$ tree consistent
	with $\cE$ and, clearly, it has its root labeled with $X$.
	Thus, the assertion follows.
\end{proof}

\begin{figure}
	\centering
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
		    level 1/.style={sibling distance=2.8cm, level distance=25pt},
		    level 2/.style={sibling distance=1.3cm, level distance=25pt},
				level 3/.style={sibling distance=0.8cm, level distance=25pt},
				level 4/.style={sibling distance=0.5cm, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$\redtext{X_1}$}
	      child {node [main node,inner sep=0.5pt] (2) {$\bluetext{X_2}$}
	      	child {node [main node,inner sep=0.5pt] (3) {$X_3$} {}
		      	child {node [main node,inner sep=0.5pt] (4) {$X_4$}
		      		child {node [rectangle,draw] (5) {}}
		      		child {node [rectangle,draw] (6) {}}
						}
		      	child {node [rectangle,draw] (7) {}}
					}
	      	child {node [main node,inner sep=0.5pt] (8) {$X_4$} {}
		      	child {node [rectangle,draw] (9) {}}
		      	child {node [main node,inner sep=0.5pt] (10) {$X_3$}
		      		child {node [rectangle,draw] (11) {}}
		      		child {node [rectangle,draw] (12) {}}
						}
					}
	      }
	      child {node [main node,inner sep=0.5pt] (13) {$X_3$}
	      	child {node [main node,inner sep=0.5pt] (14) {$\bluetext{X_2}$} {}
		      	child {node [rectangle,draw] (15) {}}
		      	child {node [rectangle,draw] (16) {}}
					}
	      	child {node [main node,inner sep=0.5pt] (17) {$\bluetext{X_2}$} {}
		      	child {node [main node,inner sep=0.5pt] (18) {$X_4$}
		      		child {node [rectangle,draw] (19) {}}
		      		child {node [rectangle,draw] (20) {}}
						}
		      	child {node [rectangle,draw] (21) {}}
					}
				};
	  \end{tikzpicture}
		\caption{$T$ \label{fig:promote_T}}
	\end{subfigure}\\
  \begin{subfigure}[b]{0.45\textwidth}
		\centering
	  \begin{tikzpicture}[->,>=stealth',
		    level 1/.style={sibling distance=2.8cm, level distance=25pt},
		    level 2/.style={sibling distance=1.3cm, level distance=25pt},
				level 3/.style={sibling distance=0.8cm, level distance=25pt},
				level 4/.style={sibling distance=0.5cm, level distance=25pt}]
	    \node [main node,inner sep=0.5pt] (1){$\bluetext{X_2}$}
	      child {node [main node,inner sep=0.5pt] (2) {$\redtext{X_1}$}
	      	child {node [main node,inner sep=0.5pt] (3) {$X_3$} {}
		      	child {node [main node,inner sep=0.5pt] (4) {$X_4$}
		      		child {node [rectangle,draw] (5) {}}
		      		child {node [rectangle,draw] (6) {}}
						}
		      	child {node [rectangle,draw] (7) {}}
					}
	      	child {node [main node,inner sep=0.5pt] (8) {$X_3$} {}
		      	child {node [rectangle,draw] (9) {}}
		      	child {node [main node,inner sep=0.5pt] (10) {$X_4$}
		      		child {node [rectangle,draw] (11) {}}
		      		child {node [rectangle,draw] (12) {}}
						}
					}
	      }
	      child {node [main node,inner sep=0.5pt] (13) {$\redtext{X_1}$}
	      	child {node [main node,inner sep=0.5pt] (14) {$X_4$} {}
		      	child {node [rectangle,draw] (15) {}}
		      	child {node [main node,inner sep=0.5pt] (16) {$X_3$}
		      		child {node [rectangle,draw] (17) {}}
		      		child {node [rectangle,draw] (18) {}}
						}
					}
	      	child {node [main node,inner sep=0.5pt] (19) {$X_3$} {}
		      	child {node [rectangle,draw] (20) {}}
		      	child {node [rectangle,draw] (21) {}}
					}
				};
	  \end{tikzpicture}
		\caption{$T'$ \label{fig:promote_T_updated}}
	\end{subfigure}
	\caption{$X_2 \in \AI(\cE,S)$ is picked at the root \label{fig:promote}}
\end{figure}

\propref{2} clearly implies the correctness of \algref{recur_learnCI} with 
$\AI(\cE,S)$ defined as above for class $\CI$-$\FP$ and each $x_l \in 
(X_l,x_l)$ set to $1$.

\begin{thm}
Let $\cE$ be a set of examples over a set $\cI$ of binary issues.
\algref{recur_learnCI} adjusted as described above terminates and outputs
a $\CI$-$\FP$ tree $T$ consistent with $\cE$ if
and only if such a tree exists.
\end{thm}


\noindent {\bf Conditional Preference.}
For class $\CI$-$\CP$, we define that $\AI(\cE,S)$ contains issue $X \not \in \NEQ(\cE)$ if 

\noindent (4) \ for every $(\alpha,\beta,1) \in \cE^\succ$, $\alpha(X) \geq 
\beta(X)$, or for every $(\alpha,\beta,1) \in \cE^\succ$, $\alpha(X) \leq \beta(X)$.

We obtain an algorithm learning $\CI$-$\CP$ trees by using in line 4
the present definition of $\AI(\cE,S)$. In line 8, we take for 
$x_l$ either $1$ or $0$ (depending on which of the two
cases in (4) holds for $X_l$).
The correctness of this algorithm follows from a property similar to
that in \propref{2}.


\vspace{-0.1cm}
\subsection{The \tsc{SmallLearn} and \tsc{MaxLearn} Problems}

\vspace{-0.1cm}
Due to space limits, we only outline the results we have for this case. 
Both problems for the three $\CI$ classes are NP-complete. They are in 
NP since if a witness PLP-tree exists, one can modify it so that 
its size does not exceed the size of the input. Hardness of the 
\tsc{SmallLearn} problem for $\CI$ classes follows from the proof of 
\thmref{UIFP_smallest_decision}, whereas the hardness of the 
\tsc{MaxLearn} problem for $\CI$ cases follows from the proof 
by Schmitt and Martignon \shortcite{schmitt2006complexity}.


%\section{Possible Experiment Settings}
%\begin{enumerate}
%	\item Generate a random PLP-tree $T_r$, generate a set of examples $\cE$ consistent with $T_r$;
%				learn a PLP-tree $T_l$ consistent with $\cE$;
%				compare $T_r$ and $T_l$ (e.g., compute the percentage of agreed and disagreed pairs
%				\cite{conf/adt13/JG}).
%	\item Generate random consistent examples, learn a PLP-tree and predict preference on new examples
%				based on the tree learned, and compare the results against machine learning 
%				results \cite{busa2014pac}.
%	\item Generate random examples, learn a PLP-tree that satisfies many examples 
%				and predict preference on new examples
%				based on the tree learned, and compare the results against machine learning 
%				results \cite{busa2014pac}.
%\end{enumerate}


\section{Conclusions and Future Work}
We proposed a preference language, \tit{partial lexicographic preference 
trees}, \tit{PLP-trees}, as a way to represent preferences over combinatorial 
domains. For several natural classes of PLP-trees, we studied passive learning 
problems: \tsc{ConsLearn}, \tsc{SmallLearn} and \tsc{MaxLearn}. All complexity
results we obtained are summarized in tables in \figref{comp_results}. The
\tsc{ConsLearn} problem for $\UI$-$\CP$ trees is as of now unsettled. While we are aware 
of subclasses of $\UI$-$\CP$ trees for which polynomial-time algorithms are 
possible, we conjecture that in general, the problem is NP-complete.

\begin{figure}[!ht]
	\centering
  \begin{subfigure}[b]{0.25\textwidth}
		\centering
		\begin{tabular}[0.25\textwidth]{ | c | c | c | c | }
		  \hline
		     & FP & UP & CP \\
		  \hline
		  UI & P & P & \tit{NP}\\
		  \hline
		  CI & P & NPC & P  \\
		  \hline
		\end{tabular}
		\caption{\tsc{ConsLearn}}
		\label{fig:cons_learn}
	\end{subfigure}\\
  \begin{subfigure}[b]{0.25\textwidth}
		\centering
		\begin{tabular}[0.25\textwidth]{ | c | c | c | c | }
		  \hline
		     & FP & UP & CP \\
		  \hline
		  UI & NPC & NPC & NPC \\
		  \hline
		  CI & NPC & NPC & NPC \\
		  \hline
		\end{tabular}
		\caption{\tsc{SmallLearn} \& \tsc{MaxLearn}}
		\label{fig:small_max_learn}
	\end{subfigure}
	\caption{Complexity results for passive learning problems}
	\label{fig:comp_results}
\end{figure}

For the future research, we will develop good heuristics for our learning algorithms.
We will implement these algorithms handling issues of, in general, finite domains
of values, and evaluate them on both synthetic and
real-world preferential datasets. 
With PLP-trees of various classes learned, we will compare our models with
the ones learned through other learning approaches on predicting new preferences.
